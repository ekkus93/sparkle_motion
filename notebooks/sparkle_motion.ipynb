{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "768dbf3c",
   "metadata": {},
   "source": [
    "# sparkle_motion — Colab A100 runbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eda5d3",
   "metadata": {},
   "source": [
    "\n",
    "This notebook **only** supports the Google Colab A100 runtime. Every cell assumes you are in `/content` with a GPU attached and that the repo lives at `/content/sparkle_motion`.\n",
    "- Step through the cells in order; do not skip the install step.\n",
    "- All workspace data stays under `/content/<workspace_name>`; there is a single storage path for every run.\n",
    "- The smoke-test cell is a safe stub that checks imports and shows how to run the orchestrator in simulation mode once the stack is installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c497420",
   "metadata": {},
   "source": [
    "## Notes and expectations\n",
    "- Always request an A100 runtime in Colab (`Runtime → Change runtime type → GPU → A100`).\n",
    "- Keep the notebook tab in focus while installs run; cancel/restart the runtime if `pip install` fails.\n",
    "- Everything installs under `/content`; rerunning the notebook in a fresh runtime recreates the same layout automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d57d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: Guard execution environment and enforce repo clone\n",
    "from __future__ import annotations\n",
    "\n",
    "import importlib.util\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if importlib.util.find_spec(\"google.colab\") is None:\n",
    "    raise RuntimeError(\"This notebook only runs on Google Colab with an A100 GPU attached.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21793c8",
   "metadata": {},
   "source": [
    "## Pull the repository into this runtime\n",
    "Use the next cell when you open this notebook directly from GitHub in Colab. It clones (or updates) the `sparkle_motion` repo inside `/content` so every other helper has access to the full source tree. All paths in the rest of the notebook assume the repo lives at `/content/sparkle_motion`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94142526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Clone or update the sparkle_motion repo (Colab-friendly)\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "REPO_URL = \"https://github.com/ekkus93/sparkle_motion.git\"\n",
    "TARGET_DIR = Path(\"/content/sparkle_motion\").resolve()\n",
    "\n",
    "\n",
    "def _existing_repo_root(start: Path) -> Optional[Path]:\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / \"pyproject.toml\").exists():\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "\n",
    "def _run_git(*args: str) -> None:\n",
    "    subprocess.check_call([\"git\", *args])\n",
    "\n",
    "\n",
    "current_dir = Path.cwd().resolve()\n",
    "repo_root = _existing_repo_root(current_dir)\n",
    "\n",
    "if repo_root:\n",
    "    print(f\"Found existing sparkle_motion repo at {repo_root}\")\n",
    "    os.chdir(repo_root)\n",
    "else:\n",
    "    if TARGET_DIR.exists():\n",
    "        if (TARGET_DIR / \".git\").exists():\n",
    "            print(f\"Repository already present at {TARGET_DIR}; pulling latest changes...\")\n",
    "            _run_git(\"-C\", str(TARGET_DIR), \"fetch\", \"--all\", \"--prune\")\n",
    "            _run_git(\"-C\", str(TARGET_DIR), \"pull\", \"--ff-only\")\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"{TARGET_DIR} exists but is not a git repository. Delete it or update TARGET_DIR.\"\n",
    "            )\n",
    "    else:\n",
    "        print(f\"Cloning {REPO_URL} into {TARGET_DIR} ...\")\n",
    "        _run_git(\"clone\", REPO_URL, str(TARGET_DIR))\n",
    "    os.chdir(TARGET_DIR)\n",
    "    repo_root = TARGET_DIR\n",
    "\n",
    "REPO_ROOT = Path.cwd().resolve()\n",
    "globals()[\"REPO_ROOT\"] = REPO_ROOT\n",
    "os.environ[\"SPARKLE_MOTION_REPO_ROOT\"] = str(REPO_ROOT)\n",
    "print(f\"Working directory switched to {REPO_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec1b3f9",
   "metadata": {},
   "source": [
    "## Configure workspace and load `.env`\n",
    "This cell creates your workspace folder under `/content/<workspace_name>`, records the Hugging Face model list, and loads secrets from the repo checkout (`/content/sparkle_motion/.env`). Before running it, create `/content/sparkle_motion/.env` (same folder as the cloned repo) and drop your keys there. The cell will refuse to continue until that file exists and will print the absolute path it just loaded.\n",
    "\n",
    "### Required entries in `/content/sparkle_motion/.env`\n",
    "| Key | Purpose | Example value | Notes |\n",
    "| --- | --- | --- | --- |\n",
    "| `ADK_PROJECT` | ADK project slug | `sparkle-motion` | Used by the CLI + FunctionTools. |\n",
    "| `ADK_API_KEY` | API key for that project | `sk-************************` | Follow the steps below to create it. |\n",
    "| `HF_TOKEN` | Hugging Face token | `hf_************************` | Needed for private model downloads; leave blank if all repos are public. |\n",
    "| `SPARKLE_DB_PATH` | SQLite path for RunRegistry + dedupe | `/content/SparkleMotion/sparkle.db` | Any absolute path works; the parent folder must exist. Run `touch /content/SparkleMotion/sparkle.db` once if the file is missing. |\n",
    "| `ARTIFACTS_DIR` | Root folder where agents publish artifacts | `/content/SparkleMotion/artifacts` | Create the directory up front (`mkdir -p /content/SparkleMotion/artifacts`). The CLI + FunctionTools will write manifests/files here. |\n",
    "| `GOOGLE_ADK_PROFILE` | ToolRegistry profile to use | `local-colab` | Matches the endpoints defined in `configs/tool_registry.yaml`. Keep this if you are running inside Colab. |\n",
    "\n",
    "Feel free to add other knobs (e.g., `SPARKLE_RECENT_INDEX_SQLITE`, `SMOKE_*` flags) but the table above is the minimum set the notebook expects.\n",
    "\n",
    "How to create an `ADK_API_KEY`:\n",
    "1. Visit https://aistudio.google.com and sign in with the project owner account.\n",
    "2. Open **Projects → sparkle-motion** (or your project).\n",
    "3. Click **API Keys → Create key**, give it a label, and copy the token.\n",
    "4. Edit `/content/sparkle_motion/.env` and add `ADK_API_KEY=<copied token>`.\n",
    "\n",
    "Once the file exists, rerun the cell anytime—it always loads from `/content/sparkle_motion/.env` and keeps the workspace rooted in `/content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d382dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configure workspace and load secrets\n",
    "from __future__ import annotations\n",
    "\n",
    "import importlib.util\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "if \"REPO_ROOT\" not in globals():\n",
    "    raise RuntimeError(\"REPO_ROOT is undefined. Run the repo clone cell before configuring the workspace.\")\n",
    "\n",
    "REPO_ROOT = Path(globals()[\"REPO_ROOT\"]).resolve()\n",
    "\n",
    "WORKSPACE_NAME = \"SparkleMotion\"          # Folder created under /content\n",
    "HF_MODELS: List[str] = [\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    \"Wan-AI/Wan2.1-I2V-14B-720P\",\n",
    "    \"Wan-AI/Wan2.1-FLF2V-14B-720P-diffusers\",\n",
    "    \"ResembleAI/chatterbox\",\n",
    "]\n",
    "\n",
    "workspace_root = Path(\"/content\") / WORKSPACE_NAME\n",
    "workspace_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "globals()[\"WORKSPACE_ROOT\"] = workspace_root\n",
    "globals()[\"HF_MODELS\"] = HF_MODELS\n",
    "globals()[\"WORKSPACE_NAME\"] = WORKSPACE_NAME\n",
    "\n",
    "final_video_dir = workspace_root / \"final_videos\"\n",
    "final_video_dir.mkdir(parents=True, exist_ok=True)\n",
    "globals()[\"FINAL_VIDEO_DIR\"] = str(final_video_dir)\n",
    "os.environ[\"FINAL_VIDEO_DIR\"] = str(final_video_dir)\n",
    "\n",
    "model_notes_path = (REPO_ROOT / \"docs\" / \"MODEL_INSTALL_NOTES.md\").resolve()\n",
    "if not model_notes_path.exists():\n",
    "    raise FileNotFoundError(f\"MODEL_INSTALL_NOTES not found at {model_notes_path}\")\n",
    "\n",
    "print(f\"Configured workspace '{WORKSPACE_NAME}' → {workspace_root}\")\n",
    "print(f\"Models to manage: {HF_MODELS or '[none specified]'}\")\n",
    "print(f\"Model install reference: {model_notes_path}\")\n",
    "print(f\"FINAL_VIDEO_DIR → {final_video_dir}\")\n",
    "\n",
    "env_path = REPO_ROOT / \".env\"\n",
    "if not env_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"No .env file found at {env_path}. Create it inside the repo checkout before running this cell.\"\n",
    "    )\n",
    "\n",
    "print(f\"Using secrets file at {env_path}\")\n",
    "\n",
    "if importlib.util.find_spec(\"dotenv\") is None:\n",
    "    print(\"Installing python-dotenv ...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"python-dotenv\"])\n",
    "\n",
    "from dotenv import load_dotenv  # type: ignore\n",
    "\n",
    "load_dotenv(env_path, override=True)\n",
    "globals()[\"DOTENV_LOADED\"] = True\n",
    "print(f\"Loaded environment variables from {env_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b39555",
   "metadata": {},
   "source": [
    "## Auto-create artifacts + DB paths\n",
    "Run this right after loading `.env`. It reads the `ARTIFACTS_DIR` and `SPARKLE_DB_PATH` values you just imported, ensures their parent directories exist, and creates the SQLite file if it is missing. Rerun anytime you change those paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1eb507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3a: Ensure ARTIFACTS_DIR and SPARKLE_DB_PATH exist\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if not globals().get(\"DOTENV_LOADED\"):\n",
    "    raise RuntimeError(\"Environment variables are missing. Run the .env loader first.\")\n",
    "if \"REPO_ROOT\" not in globals():\n",
    "    raise RuntimeError(\"REPO_ROOT is undefined. Run the repo clone cell before continuing.\")\n",
    "\n",
    "repo_root = Path(globals()[\"REPO_ROOT\"]).resolve()\n",
    "env_path = repo_root / \".env\"\n",
    "\n",
    "\n",
    "def _require_absolute_env(key: str) -> Path:\n",
    "    raw_value = os.environ.get(key, \"\").strip()\n",
    "    if not raw_value:\n",
    "        raise RuntimeError(f\"{key} is missing or empty in {env_path}. Update the file and rerun the loader cell.\")\n",
    "    path = Path(raw_value).expanduser()\n",
    "    if not path.is_absolute():\n",
    "        raise RuntimeError(f\"{key} must be an absolute path. Current value: {raw_value}\")\n",
    "    return path.resolve(strict=False)\n",
    "\n",
    "\n",
    "def _ensure_directory(path: Path) -> None:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Ensured directory exists: {path}\")\n",
    "\n",
    "\n",
    "artifacts_dir = _require_absolute_env(\"ARTIFACTS_DIR\")\n",
    "_ensure_directory(artifacts_dir)\n",
    "\n",
    "sparkle_db_path = _require_absolute_env(\"SPARKLE_DB_PATH\")\n",
    "_ensure_directory(sparkle_db_path.parent)\n",
    "if sparkle_db_path.exists():\n",
    "    print(f\"SQLite registry already present at {sparkle_db_path}\")\n",
    "else:\n",
    "    sparkle_db_path.touch()\n",
    "    print(f\"Initialized empty SQLite registry at {sparkle_db_path}\")\n",
    "\n",
    "os.environ[\"ARTIFACTS_DIR\"] = str(artifacts_dir)\n",
    "os.environ[\"SPARKLE_DB_PATH\"] = str(sparkle_db_path)\n",
    "\n",
    "print(\"Artifacts + database paths verified.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f3ace",
   "metadata": {},
   "source": [
    "## Prepare workspace and download models\n",
    "This cell does three things whenever you run it:\n",
    "1. Calls `scripts/colab_drive_setup.py --workspace <WORKSPACE_ROOT>` (legacy name, still handles `/content` workspaces) to create any missing folders.\n",
    "2. Downloads every repo listed in `HF_MODELS` (defined in the previous cell) using your `HF_TOKEN` from `.env`.\n",
    "3. Writes a quick health report to `outputs/colab_smoke.json` so you can confirm each model pulled down correctly.\n",
    "\n",
    "Before you press **Run**:\n",
    "- Double-check the prior cell succeeded (so `WORKSPACE_ROOT`, `HF_MODELS`, and `DOTENV_LOADED` are all set).\n",
    "- Replace entries in `HF_MODELS` with the exact Hugging Face repos you need (leave it empty only if you truly want to skip downloads).\n",
    "- Make sure your `.env` includes `HF_TOKEN` if any repos are private.\n",
    "\n",
    "Once those are ready, just run the next cell—it will stream the script output (progress + any Hugging Face errors) directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae3ee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Prepare workspace folders and download required models\n",
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Sequence\n",
    "\n",
    "if not DOTENV_LOADED:\n",
    "    raise RuntimeError(\"Environment variables are missing. Run the .env loader first.\")\n",
    "if \"WORKSPACE_ROOT\" not in globals():\n",
    "    raise RuntimeError(\"Workspace is not configured. Run the workspace configuration cell first.\")\n",
    "if \"HF_MODELS\" not in globals():\n",
    "    raise RuntimeError(\"HF_MODELS is missing. Set them in the workspace configuration cell.\")\n",
    "if \"REPO_ROOT\" not in globals():\n",
    "    raise RuntimeError(\"REPO_ROOT is undefined. Run the repo clone cell first.\")\n",
    "\n",
    "workspace_root = Path(globals()[\"WORKSPACE_ROOT\"]).resolve()\n",
    "models: Sequence[str] = globals()[\"HF_MODELS\"]\n",
    "repo_root = Path(globals()[\"REPO_ROOT\"]).resolve()\n",
    "src_path = repo_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "from sparkle_motion.colab_helper import download_model, ensure_workspace, run_smoke_check\n",
    "\n",
    "layout = ensure_workspace(workspace_root)\n",
    "print(f\"Workspace ready under {layout.root}\")\n",
    "print(f\"models -> {layout.models}\")\n",
    "print(f\"assets -> {layout.assets}\")\n",
    "print(f\"outputs -> {layout.outputs}\")\n",
    "\n",
    "for repo_id in models:\n",
    "    target_dir = layout.models / repo_id.replace(\"/\", \"__\")\n",
    "    print(f\"\\nDownloading {repo_id} into {target_dir}\")\n",
    "    download_model(repo_id=repo_id, target_dir=target_dir)\n",
    "\n",
    "smoke_path = run_smoke_check(layout, models=models)\n",
    "print(f\"\\nSmoke report written to {smoke_path}\")\n",
    "print(\"Workspace ready and all models downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fbe094",
   "metadata": {},
   "source": [
    "## Install Wav2Lip (lipsync helper)\n",
    "This step vendors the upstream [Wav2Lip](https://github.com/Rudrabha/Wav2Lip) repo into your workspace, installs its Python requirements, and downloads the `wav2lip_gan.pth` checkpoint the lipsync stage needs. Run it after the Hugging Face download cell so the workspace already exists.\n",
    "\n",
    "The cell below will:\n",
    "- Clone (or update) the Wav2Lip repo under your `WORKSPACE_ROOT`.\n",
    "- Install `requirements.txt` with `pip`.\n",
    "- Fetch `wav2lip_gan.pth` into `Wav2Lip/checkpoints/`.\n",
    "\n",
    "It also exports `WAV2LIP_ROOT` so downstream helpers pick up the local path automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4b: Install Wav2Lip lipsync helper\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "if \"WORKSPACE_ROOT\" not in globals():\n",
    "    raise RuntimeError(\"Workspace is not configured. Run the workspace configuration cell first.\")\n",
    "\n",
    "workspace_root = Path(globals()[\"WORKSPACE_ROOT\"]).resolve()\n",
    "wav2lip_root = workspace_root / \"Wav2Lip\"\n",
    "repo_url = \"https://github.com/Rudrabha/Wav2Lip.git\"\n",
    "checkpoint_dir = wav2lip_root / \"checkpoints\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "checkpoint_path = checkpoint_dir / \"wav2lip_gan.pth\"\n",
    "checkpoint_url = os.environ.get(\"WAV2LIP_CHECKPOINT_URL\", \"https://storage.googleapis.com/vit-wav2lip/Wav2Lip_gan.pth\")\n",
    "curated_packages = os.environ.get(\"WAV2LIP_PIP_PACKAGES\")\n",
    "CURATED_WAV2LIP_PACKAGES: tuple[str, ...] = (\n",
    "    \"numpy>=1.23,<2.0\",\n",
    "    \"scipy>=1.11\",\n",
    "    \"opencv-python>=4.8\",\n",
    "    \"librosa>=0.10\",\n",
    "    \"matplotlib>=3.8\",\n",
    "    \"pydub>=0.25\",\n",
    "    \"tqdm>=4.66\",\n",
    "    \"numba>=0.58\",\n",
    "    \"ffmpeg-python>=0.2\",\n",
    "    \"gdown>=4.7\",\n",
    "    \"soundfile>=0.12\",\n",
    "    \"torchlibrosa>=0.1.0\",\n",
    ")\n",
    "\n",
    "def _install_packages(packages: Iterable[str]) -> None:\n",
    "    packages = list(packages)\n",
    "    if not packages:\n",
    "        return\n",
    "    print(f\"Installing curated Wav2Lip dependencies: {packages}\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *packages])\n",
    "\n",
    "if not wav2lip_root.exists():\n",
    "    print(f\"Cloning Wav2Lip into {wav2lip_root} ...\")\n",
    "    subprocess.check_call([\"git\", \"clone\", \"--depth\", \"1\", repo_url, str(wav2lip_root)])\n",
    "else:\n",
    "    print(f\"Wav2Lip already exists at {wav2lip_root}; pulling latest changes.\")\n",
    "    subprocess.check_call([\"git\", \"-C\", str(wav2lip_root), \"pull\", \"--ff-only\"])\n",
    "\n",
    "requirements_file = wav2lip_root / \"requirements.txt\"\n",
    "requirements_installed = False\n",
    "if requirements_file.exists():\n",
    "    print(\"Installing Wav2Lip Python requirements ...\")\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(requirements_file)])\n",
    "        requirements_installed = True\n",
    "    except subprocess.CalledProcessError as exc:\n",
    "        print(\"Legacy requirements.txt failed (likely incompatible pins). Falling back to curated dependency set.\")\n",
    "        if curated_packages:\n",
    "            curated = tuple(pkg.strip() for pkg in curated_packages.split(\",\") if pkg.strip())\n",
    "        else:\n",
    "            curated = CURATED_WAV2LIP_PACKAGES\n",
    "        _install_packages(curated)\n",
    "else:\n",
    "    print(\"requirements.txt not found; skipping pip install. Installing curated dependency set instead.\")\n",
    "    if curated_packages:\n",
    "        curated = tuple(pkg.strip() for pkg in curated_packages.split(\",\") if pkg.strip())\n",
    "    else:\n",
    "        curated = CURATED_WAV2LIP_PACKAGES\n",
    "    _install_packages(curated)\n",
    "\n",
    "if not checkpoint_path.exists():\n",
    "    print(f\"Downloading Wav2Lip checkpoint from {checkpoint_url} ...\")\n",
    "    with urllib.request.urlopen(checkpoint_url) as response, checkpoint_path.open(\"wb\") as handle:\n",
    "        shutil.copyfileobj(response, handle)\n",
    "else:\n",
    "    print(f\"Checkpoint already present at {checkpoint_path}; skipping download.\")\n",
    "\n",
    "globals()[\"WAV2LIP_ROOT\"] = str(wav2lip_root)\n",
    "os.environ[\"WAV2LIP_ROOT\"] = str(wav2lip_root)\n",
    "print(f\"WAV2LIP_ROOT set to {wav2lip_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff41845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Install requirements from requirements-ml.txt\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if \"REPO_ROOT\" not in globals():\n",
    "    raise RuntimeError(\"REPO_ROOT is undefined. Run the repo clone cell before installing dependencies.\")\n",
    "\n",
    "req_path = Path(globals()[\"REPO_ROOT\"]).resolve() / \"requirements-ml.txt\"\n",
    "if not req_path.exists():\n",
    "    raise FileNotFoundError(f\"requirements-ml.txt not found at {req_path}\")\n",
    "\n",
    "print(\"Installing Python dependencies (this may take several minutes)...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_path)])\n",
    "print(\"Dependency install complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6a038a",
   "metadata": {},
   "source": [
    "## Colab preflight helper\n",
    "Right after the installs finish, run the next cell to make sure this runtime is ready to go.\n",
    "\n",
    "- Press **Run** and wait a few seconds.\n",
    "- It checks that your `.env` loaded, `/content` is writable, you have enough disk/GPU, and the helper servers respond.\n",
    "- You’ll get a short list with ✅/❌ results. If a line fails, the message tells you what to fix; rerun the cell once it’s resolved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea93377",
   "metadata": {},
   "source": [
    "## Start the local servers before preflight\n",
    "Use these buttons right after the installs finish. Start ScriptAgent (port 5001) and ProductionAgent (port 5008) here so the preflight checks and control panel have live FastAPI endpoints. Stop them when you wrap up the session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b22185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import signal\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, NamedTuple\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "except ImportError as exc:  # pragma: no cover - notebook utility guard\n",
    "    raise RuntimeError(\"ipywidgets is required for the Workflow Agent controls\") from exc\n",
    "\n",
    "\n",
    "class ServerProcess(NamedTuple):\n",
    "    process: subprocess.Popen\n",
    "    log_path: Path\n",
    "    log_handle: object\n",
    "\n",
    "\n",
    "SERVER_CONFIGS: Dict[str, Dict[str, object]] = {\n",
    "    \"script_agent\": {\n",
    "        \"app\": \"sparkle_motion.function_tools.script_agent.entrypoint:app\",\n",
    "        \"port\": 5001,\n",
    "    },\n",
    "    \"production_agent\": {\n",
    "        \"app\": \"sparkle_motion.function_tools.production_agent.entrypoint:app\",\n",
    "        \"port\": 5008,\n",
    "    },\n",
    "}\n",
    "\n",
    "RUNNING_SERVERS: Dict[str, ServerProcess] = {}\n",
    "\n",
    "PYTHONPATH = os.pathsep.join(\n",
    "    filter(None, {str(Path(REPO_ROOT)), str(Path(REPO_ROOT) / \"src\"), os.environ.get(\"PYTHONPATH\", \"\")})\n",
    ")\n",
    "\n",
    "\n",
    "def _server_cmd(name: str) -> list[str]:\n",
    "    cfg = SERVER_CONFIGS[name]\n",
    "    return [\n",
    "        sys.executable,\n",
    "        \"-m\",\n",
    "        \"uvicorn\",\n",
    "        cfg[\"app\"],\n",
    "        \"--host\",\n",
    "        os.environ.get(\"WORKFLOW_AGENT_HOST\", \"127.0.0.1\"),\n",
    "        \"--port\",\n",
    "        str(cfg[\"port\"]),\n",
    "        \"--no-access-log\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def start_server(name: str) -> None:\n",
    "    server = RUNNING_SERVERS.get(name)\n",
    "    if server and server.process.poll() is None:\n",
    "        raise RuntimeError(f\"{name} already running (pid={server.process.pid})\")\n",
    "    env = os.environ.copy()\n",
    "    env[\"PYTHONPATH\"] = PYTHONPATH\n",
    "    log_path = Path(REPO_ROOT) / \"tmp\" / f\"{name}.log\"\n",
    "    log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    log_handle = open(log_path, \"ab\", buffering=0)\n",
    "    proc = subprocess.Popen(\n",
    "        _server_cmd(name),\n",
    "        env=env,\n",
    "        stdout=log_handle,\n",
    "        stderr=subprocess.STDOUT,\n",
    "    )\n",
    "    RUNNING_SERVERS[name] = ServerProcess(process=proc, log_path=log_path, log_handle=log_handle)\n",
    "    print(f\"Started {name} on port {SERVER_CONFIGS[name]['port']} (pid={proc.pid}). Logs → {log_path}\")\n",
    "\n",
    "\n",
    "def stop_server(name: str, *, sig: int = signal.SIGTERM) -> None:\n",
    "    server = RUNNING_SERVERS.get(name)\n",
    "    if not server:\n",
    "        print(f\"{name} has not been started from this notebook.\")\n",
    "        return\n",
    "    proc = server.process\n",
    "    if proc.poll() is not None:\n",
    "        print(f\"{name} already stopped (pid={proc.pid}).\")\n",
    "    else:\n",
    "        proc.send_signal(sig)\n",
    "        try:\n",
    "            proc.wait(timeout=10)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            proc.kill()\n",
    "            proc.wait()\n",
    "        print(f\"Stopped {name} (pid={proc.pid}).\")\n",
    "    server.log_handle.close()\n",
    "    RUNNING_SERVERS.pop(name, None)\n",
    "\n",
    "\n",
    "def list_servers() -> Dict[str, str]:\n",
    "    status = {}\n",
    "    for name in SERVER_CONFIGS:\n",
    "        proc = RUNNING_SERVERS.get(name)\n",
    "        if proc and proc.process.poll() is None:\n",
    "            status[name] = f\"running (pid={proc.process.pid})\"\n",
    "        else:\n",
    "            status[name] = \"stopped\"\n",
    "    return status\n",
    "\n",
    "\n",
    "server_selector = widgets.Dropdown(options=list(SERVER_CONFIGS.keys()), description=\"Server\")\n",
    "start_button = widgets.Button(description=\"Start\", button_style=\"success\")\n",
    "stop_button = widgets.Button(description=\"Stop\", button_style=\"danger\")\n",
    "status_button = widgets.Button(description=\"Status\", button_style=\"info\")\n",
    "output = widgets.Output()\n",
    "\n",
    "\n",
    "def _handle_start(_: widgets.Button) -> None:\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        try:\n",
    "            start_server(server_selector.value)\n",
    "        except Exception as exc:  # pragma: no cover - notebook UX guard\n",
    "            print(f\"Failed to start {server_selector.value}: {exc}\")\n",
    "\n",
    "\n",
    "def _handle_stop(_: widgets.Button) -> None:\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        stop_server(server_selector.value)\n",
    "\n",
    "\n",
    "def _handle_status(_: widgets.Button) -> None:\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        for name, state in list_servers().items():\n",
    "            print(f\"{name}: {state}\")\n",
    "\n",
    "\n",
    "start_button.on_click(_handle_start)\n",
    "stop_button.on_click(_handle_stop)\n",
    "status_button.on_click(_handle_status)\n",
    "\n",
    "controls = widgets.HBox([server_selector, start_button, stop_button, status_button])\n",
    "display(widgets.VBox([controls, output]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d1cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Run the consolidated Colab preflight checks\n",
    "from pathlib import Path\n",
    "from sparkle_motion.notebook_preflight import format_report, run_preflight_checks\n",
    "\n",
    "if \"WORKSPACE_ROOT\" not in globals():\n",
    "    raise RuntimeError(\"WORKSPACE_ROOT is undefined. Run the workspace configuration cell first.\")\n",
    "if \"REPO_ROOT\" not in globals():\n",
    "    raise RuntimeError(\"REPO_ROOT is undefined. Run the repo clone cell first.\")\n",
    "\n",
    "workspace_root = Path(globals()[\"WORKSPACE_ROOT\"]).resolve()\n",
    "repo_root = Path(globals()[\"REPO_ROOT\"]).resolve()\n",
    "\n",
    "preflight_results = run_preflight_checks(\n",
    "    requirements_path=repo_root / \"requirements-ml.txt\",\n",
    "    mount_point=workspace_root,\n",
    "    workspace_dir=workspace_root,\n",
    "    ready_endpoints=(\n",
    "        \"http://localhost:5001/ready\",\n",
    "        \"http://localhost:5008/ready\",\n",
    "    ),\n",
    "    pip_mode=\"install\",\n",
    "    require_drive=False,\n",
    "    skip_gpu_checks=False,\n",
    ")\n",
    "\n",
    "print(format_report(preflight_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6b: Inspect smoke artifact with per-model status\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "if \"WORKSPACE_ROOT\" not in globals():\n",
    "    raise RuntimeError(\"WORKSPACE_ROOT is undefined. Run the workspace configuration cell first.\")\n",
    "\n",
    "workspace_root = Path(globals()[\"WORKSPACE_ROOT\"]).resolve()\n",
    "smoke_path = workspace_root / \"outputs\" / \"colab_smoke.json\"\n",
    "if smoke_path.exists():\n",
    "    data = json.loads(smoke_path.read_text(encoding=\"utf-8\"))\n",
    "    status = \"OK\" if data.get(\"ok\") else \"FAILED\"\n",
    "    print(f\"Smoke status: {status}\")\n",
    "    for model in data.get(\"models\", []):\n",
    "        sample = model.get(\"sample_file\") or \"n/a\"\n",
    "        print(\n",
    "            f\"- {model['repo_id']}: {model['status']} \"\n",
    "            f\"({model.get('files_present', 0)} files, sample={sample})\"\n",
    "        )\n",
    "else:\n",
    "    print(\"Smoke artifact not found yet — run the helper above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909e89ba",
   "metadata": {},
   "source": [
    "## Artifact Service controls\n",
    "Use these buttons when you want the notebook to save artifacts straight to your `/content/<workspace_name>` workspace without talking to the hosted service.\n",
    "\n",
    "1. **Show settings** – prints the environment variables (paths, token) so you can reuse them in another shell if needed.\n",
    "2. **Start service** – launches the local Artifact Service in the background so stages can read/write files right away.\n",
    "3. **Check health** – confirms the service is responding before you point anything at it.\n",
    "\n",
    "Start it once per session, then leave it running until you’re done. Everything lands inside your `/content/<workspace_name>` tree so runs stay self-contained in this runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd26b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import signal\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "except ImportError as exc:  # pragma: no cover - notebook helper\n",
    "    raise RuntimeError(\"ipywidgets is required for the filesystem shim controls\") from exc\n",
    "\n",
    "from sparkle_motion.filesystem_artifacts.config import DEFAULT_ARTIFACTS_FS_BASE_URL\n",
    "\n",
    "if \"WORKSPACE_ROOT\" not in globals():\n",
    "    raise RuntimeError(\"WORKSPACE_ROOT is undefined. Run the workspace configuration cell first.\")\n",
    "\n",
    "workspace_root = Path(globals()[\"WORKSPACE_ROOT\"]).resolve()\n",
    "REPO_ROOT = Path(os.environ.get(\"SPARKLE_MOTION_REPO_ROOT\", Path.cwd())).resolve()\n",
    "SRC_PATH = REPO_ROOT / \"src\"\n",
    "_base_url = os.environ.get(\"ARTIFACTS_FS_BASE_URL\", DEFAULT_ARTIFACTS_FS_BASE_URL)\n",
    "_parsed_base = urlparse(_base_url)\n",
    "_default_host = _parsed_base.hostname or \"127.0.0.1\"\n",
    "_default_port = _parsed_base.port or (443 if _parsed_base.scheme == \"https\" else 80)\n",
    "FS_SHIM_HOST = os.environ.get(\"FILESYSTEM_SHIM_HOST\", _default_host)\n",
    "FS_SHIM_PORT = int(os.environ.get(\"FILESYSTEM_SHIM_PORT\") or _default_port)\n",
    "FS_ROOT = (workspace_root / \"artifacts_fs\").resolve()\n",
    "FS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "_shim_process: subprocess.Popen | None = None\n",
    "_shim_log_handle = None\n",
    "_shim_log_path = REPO_ROOT / \"tmp\" / \"filesystem_shim.log\"\n",
    "_shim_log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def _shim_env() -> Dict[str, str]:\n",
    "    env = os.environ.copy()\n",
    "    env.setdefault(\"ARTIFACTS_BACKEND\", \"filesystem\")\n",
    "    env.setdefault(\"ARTIFACTS_FS_ROOT\", str(FS_ROOT))\n",
    "    env.setdefault(\"ARTIFACTS_FS_INDEX\", str(FS_ROOT / \"index.db\"))\n",
    "    env.setdefault(\"ARTIFACTS_FS_BASE_URL\", f\"http://{FS_SHIM_HOST}:{FS_SHIM_PORT}\")\n",
    "    env.setdefault(\"ARTIFACTS_FS_TOKEN\", env.get(\"ARTIFACTS_FS_TOKEN\") or \"local-fs-token\")\n",
    "    env[\"PYTHONPATH\"] = os.pathsep.join(filter(None, {str(REPO_ROOT), str(SRC_PATH), env.get(\"PYTHONPATH\", \"\")}))\n",
    "    return env\n",
    "\n",
    "\n",
    "def _start_filesystem_shim(_: widgets.Button | None = None) -> None:\n",
    "    global _shim_process, _shim_log_handle\n",
    "    if _shim_process and _shim_process.poll() is None:\n",
    "        with shim_output:\n",
    "            shim_output.clear_output()\n",
    "            print(f\"Filesystem shim already running on {FS_SHIM_HOST}:{FS_SHIM_PORT} (pid={_shim_process.pid}).\")\n",
    "        return\n",
    "    env = _shim_env()\n",
    "    _shim_log_handle = open(_shim_log_path, \"ab\", buffering=0)\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        \"scripts/filesystem_artifacts.py\",\n",
    "        \"serve\",\n",
    "        \"--host\",\n",
    "        FS_SHIM_HOST,\n",
    "        \"--port\",\n",
    "        str(FS_SHIM_PORT),\n",
    "    ]\n",
    "    _shim_process = subprocess.Popen(cmd, env=env, stdout=_shim_log_handle, stderr=subprocess.STDOUT)\n",
    "    with shim_output:\n",
    "        shim_output.clear_output()\n",
    "        print(f\"Started filesystem shim on {FS_SHIM_HOST}:{FS_SHIM_PORT} (pid={_shim_process.pid}).\")\n",
    "        print(f\"Logs → {_shim_log_path}\")\n",
    "\n",
    "\n",
    "def _stop_filesystem_shim(_: widgets.Button | None = None) -> None:\n",
    "    global _shim_process, _shim_log_handle\n",
    "    if not _shim_process:\n",
    "        with shim_output:\n",
    "            shim_output.clear_output()\n",
    "            print(\"Filesystem shim has not been started from this notebook.\")\n",
    "        return\n",
    "    if _shim_process.poll() is None:\n",
    "        _shim_process.send_signal(signal.SIGTERM)\n",
    "        try:\n",
    "            _shim_process.wait(timeout=10)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            _shim_process.kill()\n",
    "            _shim_process.wait()\n",
    "    if _shim_log_handle:\n",
    "        _shim_log_handle.close()\n",
    "        _shim_log_handle = None\n",
    "    with shim_output:\n",
    "        shim_output.clear_output()\n",
    "        print(\"Filesystem shim stopped.\")\n",
    "    _shim_process = None\n",
    "\n",
    "\n",
    "def _shim_status(_: widgets.Button | None = None) -> None:\n",
    "    with shim_output:\n",
    "        shim_output.clear_output()\n",
    "        if _shim_process and _shim_process.poll() is None:\n",
    "            print(f\"Running on {FS_SHIM_HOST}:{FS_SHIM_PORT} (pid={_shim_process.pid}).\")\n",
    "        else:\n",
    "            print(\"Filesystem shim is stopped.\")\n",
    "\n",
    "\n",
    "def _print_env_exports(_: widgets.Button | None = None) -> None:\n",
    "    env = _shim_env()\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        \"scripts/filesystem_artifacts.py\",\n",
    "        \"env\",\n",
    "        \"--shell\",\n",
    "        \"bash\",\n",
    "        \"--root\",\n",
    "        env[\"ARTIFACTS_FS_ROOT\"],\n",
    "        \"--index\",\n",
    "        env[\"ARTIFACTS_FS_INDEX\"],\n",
    "        \"--token\",\n",
    "        env[\"ARTIFACTS_FS_TOKEN\"],\n",
    "        \"--emit-token\",\n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, env=env)\n",
    "    with env_output:\n",
    "        env_output.clear_output()\n",
    "        if result.returncode == 0:\n",
    "            print(result.stdout.strip())\n",
    "        else:\n",
    "            print(result.stderr or result.stdout)\n",
    "\n",
    "\n",
    "def _health_probe(_: widgets.Button | None = None) -> None:\n",
    "    env = _shim_env()\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        \"scripts/filesystem_artifacts.py\",\n",
    "        \"health\",\n",
    "        \"--url\",\n",
    "        f\"http://{FS_SHIM_HOST}:{FS_SHIM_PORT}/healthz\",\n",
    "        \"--token\",\n",
    "        env[\"ARTIFACTS_FS_TOKEN\"],\n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, env=env)\n",
    "    with env_output:\n",
    "        env_output.clear_output()\n",
    "        if result.returncode == 0:\n",
    "            print(result.stdout.strip())\n",
    "        else:\n",
    "            print(result.stderr or result.stdout)\n",
    "\n",
    "\n",
    "start_button = widgets.Button(description=\"Start shim\", button_style=\"success\")\n",
    "stop_button = widgets.Button(description=\"Stop shim\", button_style=\"danger\")\n",
    "status_button = widgets.Button(description=\"Status\", button_style=\"info\")\n",
    "env_button = widgets.Button(description=\"Show env exports\")\n",
    "health_button = widgets.Button(description=\"Probe /healthz\", icon=\"heartbeat\")\n",
    "shim_output = widgets.Output()\n",
    "env_output = widgets.Output()\n",
    "\n",
    "start_button.on_click(_start_filesystem_shim)\n",
    "stop_button.on_click(_stop_filesystem_shim)\n",
    "status_button.on_click(_shim_status)\n",
    "env_button.on_click(_print_env_exports)\n",
    "health_button.on_click(_health_probe)\n",
    "\n",
    "controls_row = widgets.HBox([start_button, stop_button, status_button, env_button, health_button])\n",
    "display(widgets.VBox([controls_row, shim_output, env_output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08966a60",
   "metadata": {},
   "source": [
    "## Control panel: type your prompt here\n",
    "This is the UI you actually use to make a movie. When you run the next cell it pops up a panel with two text boxes: **Title** and **Prompt**. Type whatever story prompt you want in that Prompt box.\n",
    "\n",
    "1. Type your idea in the Prompt box (add a short title if you want).\n",
    "2. Click **Generate Plan**. That sends your prompt to ScriptAgent and shows the plan it creates.\n",
    "3. Set the Mode dropdown to `Run` and click **Run Production**. That hands the plan to ProductionAgent so it can build the video.\n",
    "\n",
    "The panel keeps the latest run ID so the later cells (status, artifacts, final video download) know what to show. Nothing else in this notebook collects your prompt, so always start here when you want Sparkle Motion to generate a film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da3395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "REPO_ROOT = Path(os.environ.get(\"SPARKLE_MOTION_REPO_ROOT\", Path.cwd())).resolve()\n",
    "globals()[\"REPO_ROOT\"] = REPO_ROOT\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.append(str(REPO_ROOT))\n",
    "SRC_PATH = REPO_ROOT / \"src\"\n",
    "if str(SRC_PATH) not in sys.path:\n",
    "    sys.path.append(str(SRC_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickstart cell: import and display the ipywidgets control panel\n",
    "from notebooks.control_panel import create_control_panel\n",
    "\n",
    "print(\"Launching control panel with endpoints from configs/tool_registry.yaml (profile='local-colab').\")\n",
    "control_panel = create_control_panel()\n",
    "control_panel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ab0c57",
   "metadata": {},
   "source": [
    "## Final video preview & download\n",
    "Finished a run? Use this helper to grab the final MP4. It pulls the finished video, shows it right in the notebook, and, if you’re on Colab, pops up the download dialog so you can save it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4be777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: override FINAL_VIDEO_DIR after changing workspace layout\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if \"WORKSPACE_ROOT\" not in globals():\n",
    "    raise RuntimeError(\"WORKSPACE_ROOT is undefined. Run the workspace configuration cell first.\")\n",
    "\n",
    "CUSTOM_FINAL_VIDEO_SUBDIR = \"final_videos\"  # change to another folder name if desired\n",
    "workspace_root = Path(globals()[\"WORKSPACE_ROOT\"]).resolve()\n",
    "final_dir = (workspace_root / CUSTOM_FINAL_VIDEO_SUBDIR).resolve()\n",
    "final_dir.mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"FINAL_VIDEO_DIR\"] = str(final_dir)\n",
    "print(f\"FINAL_VIDEO_DIR reset to {final_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6c6576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Final deliverable helper (finalize)\n",
    "import importlib\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import httpx\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "from notebooks import preview_helpers\n",
    "from sparkle_motion import tool_registry\n",
    "\n",
    "\n",
    "def _production_agent_base() -> str:\n",
    "    env_override = os.environ.get(\"PRODUCTION_AGENT_BASE\")\n",
    "    if env_override:\n",
    "        return env_override\n",
    "    info = tool_registry.get_local_endpoint_info(\"production_agent\")\n",
    "    if info:\n",
    "        return info.base_url\n",
    "    return \"http://127.0.0.1:5008\"\n",
    "\n",
    "\n",
    "PRODUCTION_AGENT_BASE = _production_agent_base()\n",
    "FINALIZE_STAGE = \"finalize\"\n",
    "DOWNLOAD_DIR = Path(os.environ.get(\"FINAL_VIDEO_DIR\", \"/content/final_videos\"))\n",
    "DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "colab_files = None\n",
    "try:\n",
    "    _colab_spec = importlib.util.find_spec(\"google.colab.files\")\n",
    "except ModuleNotFoundError:\n",
    "    _colab_spec = None\n",
    "if _colab_spec:\n",
    "    colab_files = importlib.import_module(\"google.colab.files\")\n",
    "\n",
    "\n",
    "def _current_run_id() -> str:\n",
    "    if \"control_panel\" in globals():\n",
    "        cp = globals()[\"control_panel\"]\n",
    "        run_value = getattr(getattr(cp, \"run_id_input\", None), \"value\", \"\")\n",
    "        if run_value and run_value.strip():\n",
    "            return run_value.strip()\n",
    "        state = getattr(cp, \"state\", None)\n",
    "        if state and getattr(state, \"last_run_request_id\", None):\n",
    "            return state.last_run_request_id\n",
    "    return os.environ.get(\"RUN_ID\", \"\").strip()\n",
    "\n",
    "\n",
    "RUN_ID = _current_run_id()\n",
    "if not RUN_ID:\n",
    "    raise RuntimeError(\n",
    "        \"Set RUN_ID (or populate control_panel.run_id_input) before running the final deliverable helper.\",\n",
    "    )\n",
    "\n",
    "\n",
    "def _fetch_stage_manifest(run_id: str) -> Dict[str, Any]:\n",
    "    stage_manifest = preview_helpers.fetch_stage_manifest(\n",
    "        base_url=PRODUCTION_AGENT_BASE,\n",
    "        run_id=run_id,\n",
    "        stage=FINALIZE_STAGE,\n",
    "    )\n",
    "    summary = preview_helpers.render_stage_summary(stage_manifest)\n",
    "    print(summary)\n",
    "    finalize_status = stage_manifest.get(\"status\") or stage_manifest.get(\"state\")\n",
    "    if finalize_status:\n",
    "        print(f\"Finalize status => {finalize_status}\")\n",
    "    return stage_manifest\n",
    "\n",
    "\n",
    "def _locate_video_final(stage_manifest: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    for entry in stage_manifest.get(\"artifacts\") or []:\n",
    "        if entry.get(\"artifact_type\") == \"video_final\":\n",
    "            return entry\n",
    "    raise RuntimeError(\n",
    "        \"No video_final artifact found. Ensure finalize succeeded or resume production_agent with resume_from='finalize'.\",\n",
    "    )\n",
    "\n",
    "\n",
    "def ensure_local_video(entry: Dict[str, Any], run_id: str) -> Path:\n",
    "    local_path = entry.get(\"local_path\")\n",
    "    if local_path:\n",
    "        candidate = Path(local_path).expanduser()\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    target = DOWNLOAD_DIR / f\"{run_id}_video_final.mp4\"\n",
    "    download_url = entry.get(\"download_url\")\n",
    "    if download_url:\n",
    "        with httpx.Client(timeout=None) as client:\n",
    "            with client.stream(\"GET\", download_url) as resp:\n",
    "                resp.raise_for_status()\n",
    "                with target.open(\"wb\") as handle:\n",
    "                    for chunk in resp.iter_bytes():\n",
    "                        handle.write(chunk)\n",
    "        return target\n",
    "    artifact_uri = entry.get(\"artifact_uri\")\n",
    "    if artifact_uri:\n",
    "        result = subprocess.run(\n",
    "            [\"adk\", \"artifacts\", \"download\", artifact_uri, str(target)],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=False,\n",
    "        )\n",
    "        if result.returncode == 0 and target.exists():\n",
    "            return target\n",
    "        print(\"ADK artifact download failed:\")\n",
    "        print(result.stderr or result.stdout)\n",
    "    raise RuntimeError(\n",
    "        \"Unable to download the final video locally. Provide download_url or local_path in the artifacts manifest.\",\n",
    "    )\n",
    "\n",
    "\n",
    "def render_preview_video(preview_entry: Optional[Dict[str, Any]]) -> None:\n",
    "    if not preview_entry:\n",
    "        print(\"No preview clip available from /artifacts preview metadata.\")\n",
    "        return\n",
    "    local_path = preview_entry.get(\"local_path\")\n",
    "    if local_path and Path(local_path).exists():\n",
    "        display(HTML(\"<strong>Preview clip</strong>\"))\n",
    "        preview_widget = preview_helpers.create_video_widget(\n",
    "            {\"local_path\": local_path, \"artifact_type\": \"preview_clip\"},\n",
    "            width=480,\n",
    "        )\n",
    "        display(preview_widget)\n",
    "        return\n",
    "    download_url = preview_entry.get(\"download_url\")\n",
    "    if download_url:\n",
    "        print(f\"Preview available remotely (download_url={download_url}).\")\n",
    "    else:\n",
    "        print(\"Preview metadata present but no local path or download URL; see artifact_uri for details.\")\n",
    "\n",
    "\n",
    "stage_manifest = _fetch_stage_manifest(RUN_ID)\n",
    "preview_entry = (stage_manifest.get(\"preview\") or {}).get(\"video\")\n",
    "final_entry = _locate_video_final(stage_manifest)\n",
    "\n",
    "render_preview_video(preview_entry)\n",
    "video_path = ensure_local_video(final_entry, RUN_ID)\n",
    "final_entry[\"local_path\"] = str(video_path)\n",
    "\n",
    "info_html = f\"\"\"\n",
    "<p><strong>Run ID:</strong> {RUN_ID}</p>\n",
    "<p><strong>Artifact URI:</strong> {final_entry.get('artifact_uri', 'n/a')}</p>\n",
    "<p><strong>Local path:</strong> {video_path}</p>\n",
    "\"\"\"\n",
    "display(HTML(info_html))\n",
    "\n",
    "display(preview_helpers.create_video_widget(final_entry, width=640))\n",
    "\n",
    "if colab_files is not None:\n",
    "    colab_files.download(str(video_path))\n",
    "else:\n",
    "    print(\"Download helper available only inside Google Colab. Share the path above manually if running elsewhere.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa4b638",
   "metadata": {},
   "source": [
    "## Artifacts viewer\n",
    "Use this helper to inspect `/artifacts` responses without leaving the notebook. It shares the `control_panel` run metadata when available, lets you scope by stage (e.g., `finalize`), and can poll automatically so new artifacts appear as production advances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb6b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4c: Artifacts viewer helper\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from typing import Any, Dict\n",
    "\n",
    "import httpx\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "from notebooks import preview_helpers\n",
    "from sparkle_motion import tool_registry\n",
    "\n",
    "\n",
    "def _production_agent_base() -> str:\n",
    "    env_override = os.environ.get(\"PRODUCTION_AGENT_BASE\")\n",
    "    if env_override:\n",
    "        return env_override\n",
    "    info = tool_registry.get_local_endpoint_info(\"production_agent\")\n",
    "    if info:\n",
    "        return info.base_url\n",
    "    return \"http://127.0.0.1:5008\"\n",
    "\n",
    "\n",
    "PRODUCTION_AGENT_BASE = _production_agent_base()\n",
    "ARTIFACTS_ENDPOINT = f\"{PRODUCTION_AGENT_BASE}/artifacts\"\n",
    "\n",
    "if \"artifact_viewer_state\" in globals():\n",
    "    existing_task = globals()[\"artifact_viewer_state\"].get(\"task\")\n",
    "    if existing_task and not existing_task.done():\n",
    "        existing_task.cancel()\n",
    "\n",
    "artifact_viewer_state = {\"task\": None}\n",
    "\n",
    "\n",
    "def _artifact_viewer_run_id() -> str:\n",
    "    if \"control_panel\" in globals():\n",
    "        cp = globals()[\"control_panel\"]\n",
    "        run_widget = getattr(cp, \"run_id_input\", None)\n",
    "        candidate = getattr(run_widget, \"value\", \"\")\n",
    "        if candidate and candidate.strip():\n",
    "            return candidate.strip()\n",
    "        state = getattr(cp, \"state\", None)\n",
    "        if state and getattr(state, \"last_run_request_id\", None):\n",
    "            return state.last_run_request_id\n",
    "    return os.environ.get(\"RUN_ID\", \"\").strip()\n",
    "\n",
    "\n",
    "def _format_status_html(message: str, *, ok: bool) -> str:\n",
    "    color = \"#3c763d\" if ok else \"#d9534f\"\n",
    "    return f\"<span style='color:{color}; font-size:0.9em;'>{message}</span>\"\n",
    "\n",
    "\n",
    "def _iter_artifacts(payload: Dict[str, Any]):\n",
    "    stages = payload.get(\"stages\") or []\n",
    "    for stage in stages:\n",
    "        for artifact in stage.get(\"artifacts\") or []:\n",
    "            yield artifact\n",
    "    for artifact in payload.get(\"artifacts\") or []:\n",
    "        yield artifact\n",
    "\n",
    "\n",
    "def _render_artifacts(payload: Dict[str, Any]) -> None:\n",
    "    artifacts = list(_iter_artifacts(payload))\n",
    "    stages = payload.get(\"stages\") or []\n",
    "    with artifacts_output:\n",
    "        artifacts_output.clear_output()\n",
    "        print(f\"Artifacts returned: {len(artifacts)}\")\n",
    "        if stages:\n",
    "            for stage in stages:\n",
    "                stage_label = stage.get(\"stage\") or stage.get(\"stage_id\") or \"stage\"\n",
    "                print(f\"\\nStage: {stage_label}\")\n",
    "                print(preview_helpers.render_stage_summary(stage))\n",
    "                previewable = [entry for entry in (stage.get(\"artifacts\") or []) if entry.get(\"local_path\")]\n",
    "                if previewable:\n",
    "                    preview_helpers.display_artifact_previews(\n",
    "                        {**stage, \"artifacts\": previewable},\n",
    "                        max_items=4,\n",
    "                        video_width=360,\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"Local previews unavailable yet; see raw payload below.\")\n",
    "        else:\n",
    "            print(\"No stage sections returned; dumping raw payload.\")\n",
    "        print(\"\\nFull payload:\\n\")\n",
    "        print(json.dumps(payload, indent=2, ensure_ascii=False))\n",
    "\n",
    "\n",
    "def _fetch_artifacts_sync(run_id: str, stage: str) -> Dict[str, Any]:\n",
    "    params = {\"run_id\": run_id}\n",
    "    if stage:\n",
    "        params[\"stage\"] = stage\n",
    "    with httpx.Client(timeout=30.0) as client:\n",
    "        resp = client.get(ARTIFACTS_ENDPOINT, params=params)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        if not isinstance(data, dict):\n",
    "            raise RuntimeError(\"Unexpected artifacts response payload\")\n",
    "        return data\n",
    "\n",
    "\n",
    "async def _fetch_artifacts_async(client: httpx.AsyncClient, run_id: str, stage: str) -> Dict[str, Any]:\n",
    "    params = {\"run_id\": run_id}\n",
    "    if stage:\n",
    "        params[\"stage\"] = stage\n",
    "    resp = await client.get(ARTIFACTS_ENDPOINT, params=params)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    if not isinstance(data, dict):\n",
    "        raise RuntimeError(\"Unexpected artifacts response payload\")\n",
    "    return data\n",
    "\n",
    "\n",
    "async def _poll_status(run_id: str, stage: str, interval_s: float, state: Dict[str, Any]) -> None:\n",
    "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "        while True:\n",
    "            try:\n",
    "                payload = await _fetch_artifacts_async(client, run_id, stage)\n",
    "            except Exception as exc:  # pragma: no cover - notebook UX guard\n",
    "                with artifacts_output:\n",
    "                    artifacts_output.clear_output()\n",
    "                    print(f\"Artifacts request failed: {exc}\")\n",
    "            else:\n",
    "                _render_artifacts(payload)\n",
    "            await asyncio.sleep(interval_s)\n",
    "            if state.get(\"task\") is None or state.get(\"cancel\"):\n",
    "                break\n",
    "\n",
    "\n",
    "def _handle_fetch(_: widgets.Button | None = None) -> None:\n",
    "    run_id = run_id_input.value.strip() or _artifact_viewer_run_id()\n",
    "    stage = stage_input.value.strip()\n",
    "    if not run_id:\n",
    "        with status_output:\n",
    "            status_output.clear_output()\n",
    "            print(\"Populate RUN_ID first (control panel or env).\")\n",
    "        return\n",
    "    try:\n",
    "        payload = _fetch_artifacts_sync(run_id, stage)\n",
    "    except Exception as exc:  # pragma: no cover - notebook UX guard\n",
    "        with status_output:\n",
    "            status_output.clear_output()\n",
    "            print(f\"Artifacts request failed: {exc}\")\n",
    "        return\n",
    "    with status_output:\n",
    "        status_output.clear_output()\n",
    "        print(_format_status_html(\"Artifacts fetched\", ok=True))\n",
    "    _render_artifacts(payload)\n",
    "\n",
    "\n",
    "def _handle_poll_change(change: Dict[str, Any]) -> None:\n",
    "    enabled = change.get(\"new\") is True\n",
    "    run_id = run_id_input.value.strip() or _artifact_viewer_run_id()\n",
    "    stage = stage_input.value.strip()\n",
    "    if enabled:\n",
    "        if not run_id:\n",
    "            with status_output:\n",
    "                status_output.clear_output()\n",
    "                print(_format_status_html(\"Populate RUN_ID before polling\", ok=False))\n",
    "            poll_toggle.value = False\n",
    "            return\n",
    "        state = artifact_viewer_state\n",
    "        state[\"cancel\"] = False\n",
    "        task = asyncio.create_task(\n",
    "            _poll_status(run_id, stage, interval_input.value, state),\n",
    "        )\n",
    "        state[\"task\"] = task\n",
    "        with status_output:\n",
    "            status_output.clear_output()\n",
    "            print(_format_status_html(\"Polling started\", ok=True))\n",
    "    else:\n",
    "        state = artifact_viewer_state\n",
    "        state[\"cancel\"] = True\n",
    "        task = state.get(\"task\")\n",
    "        if task:\n",
    "            task.cancel()\n",
    "            state[\"task\"] = None\n",
    "        with status_output:\n",
    "            status_output.clear_output()\n",
    "            print(_format_status_html(\"Polling stopped\", ok=True))\n",
    "\n",
    "\n",
    "run_id_input = widgets.Text(description=\"Run ID\", placeholder=\"Auto from control panel\")\n",
    "stage_input = widgets.Text(description=\"Stage\", placeholder=\"finalize\")\n",
    "interval_input = widgets.BoundedFloatText(value=5.0, min=2.0, max=60.0, description=\"Interval (s)\")\n",
    "fetchnow_button = widgets.Button(description=\"Fetch once\", button_style=\"primary\")\n",
    "poll_toggle = widgets.ToggleButton(description=\"Poll\", icon=\"refresh\", value=False)\n",
    "status_output = widgets.Output(layout=widgets.Layout(border=\"1px solid #ddd\", min_height=\"50px\"))\n",
    "artifacts_output = widgets.Output(layout=widgets.Layout(border=\"1px solid #ddd\", min_height=\"160px\", max_height=\"320px\", overflow=\"auto\"))\n",
    "\n",
    "fetchnow_button.on_click(_handle_fetch)\n",
    "poll_toggle.observe(_handle_poll_change, names=\"value\")\n",
    "\n",
    "controls = widgets.HBox([run_id_input, stage_input, fetchnow_button, poll_toggle, interval_input])\n",
    "\n",
    "viewer = widgets.VBox([\n",
    "    controls,\n",
    "    status_output,\n",
    "    artifacts_output,\n",
    "])\n",
    "\n",
    "display(viewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78073d",
   "metadata": {},
   "source": [
    "### Quick artifacts viewer helpers\n",
    "Use the next three cells when you need to debug the artifacts viewer:\n",
    "1. **Sync run id** — copies the current Run ID from the control panel into the viewer so both panels match.\n",
    "2. **Manual refresh** — triggers one fetch of the artifacts list so you can see the latest verification logs without waiting for auto-refresh.\n",
    "3. **Snapshot payload** — prints a JSON summary (run id, artifact count, stage names) for your notebook log or troubleshooting notes.\n",
    "Only run the ones you need; each prints its own confirmation when it completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9d3ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync artifacts viewer Run ID widget with control panel\n",
    "if \"control_panel\" in globals() and hasattr(control_panel, \"run_id_input\"):\n",
    "    run_id_input.value = control_panel.run_id_input.value\n",
    "print(\"Artifacts viewer run_id now:\", run_id_input.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdbbd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger a manual artifacts refresh for verification logs\n",
    "_handle_manual_refresh(None)\n",
    "print(\"Artifacts viewer manual refresh invoked.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e2e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture artifacts payload for notebook log (single fetch)\n",
    "import json\n",
    "current_run = run_id_input.value.strip() or _artifact_viewer_run_id()\n",
    "payload_snapshot = _fetch_artifacts_sync(current_run, stage_input.value.strip())\n",
    "stage_names = []\n",
    "for stage in payload_snapshot.get(\"stages\", []):\n",
    "    stage_names.append(stage.get(\"stage\") or stage.get(\"stage_id\"))\n",
    "print(json.dumps(\n",
    "    {\n",
    "        \"run_id\": current_run,\n",
    "        \"artifact_count\": len(list(_iter_artifacts(payload_snapshot))),\n",
    "        \"stages\": stage_names,\n",
    "    },\n",
    "    indent=2,\n",
    "    ensure_ascii=False,\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9465d",
   "metadata": {},
   "source": [
    "## Filesystem cleanup helper\n",
    "Run this cell and hit **Run prune** if you need to clear out old artifacts.\n",
    "\n",
    "- Tweak the keep limits (max size, age, or optional run IDs) if you want to be selective.\n",
    "- The cleanup log streams right under the form so you can watch what gets removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1b902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shlex\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "REPO_ROOT = Path.cwd().resolve()\n",
    "CLI_PATH = REPO_ROOT / \"scripts\" / \"filesystem_artifacts.py\"\n",
    "\n",
    "ROOT_VALUE = os.environ.get(\"ARTIFACTS_FS_ROOT\", \"\").strip()\n",
    "INDEX_VALUE = os.environ.get(\"ARTIFACTS_FS_INDEX\", \"\").strip()\n",
    "backend_value = os.environ.get(\"ARTIFACTS_BACKEND\", \"\")\n",
    "\n",
    "max_bytes_input = widgets.Text(value=\"200g\", description=\"Max bytes\")\n",
    "max_age_input = widgets.BoundedFloatText(value=14.0, min=0.0, max=365.0, step=1.0, description=\"Max age (days)\")\n",
    "min_free_input = widgets.Text(value=\"\", description=\"Min free\")\n",
    "runs_input = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder=\"Optional run IDs (one per line)\",\n",
    "    description=\"Run filter\",\n",
    "    layout=widgets.Layout(width=\"60%\", height=\"80px\"),\n",
    ")\n",
    "assume_yes_checkbox = widgets.Checkbox(value=False, description=\"Auto confirm (--yes)\", indent=False)\n",
    "run_button = widgets.Button(description=\"Run prune\", icon=\"trash\")\n",
    "status_label = widgets.HTML(\"\")\n",
    "log_output = widgets.Output(layout=widgets.Layout(border=\"1px solid #ccc\", max_height=\"260px\", overflow=\"auto\"))\n",
    "\n",
    "\n",
    "def _format_status(message: str, *, ok: bool) -> str:\n",
    "    color = \"#2e7d32\" if ok else \"#c62828\"\n",
    "    return f\"<span style='color:{color}; font-weight:bold'>{message}</span>\"\n",
    "\n",
    "\n",
    "def _build_command() -> List[str]:\n",
    "    cmd = [sys.executable, str(CLI_PATH), \"prune\"]\n",
    "    if ROOT_VALUE:\n",
    "        cmd.extend([\"--root\", ROOT_VALUE])\n",
    "    if INDEX_VALUE:\n",
    "        cmd.extend([\"--index\", INDEX_VALUE])\n",
    "    if max_bytes_input.value.strip():\n",
    "        cmd.extend([\"--max-bytes\", max_bytes_input.value.strip()])\n",
    "    if max_age_input.value and max_age_input.value > 0:\n",
    "        cmd.extend([\"--max-age-days\", str(max_age_input.value)])\n",
    "    if min_free_input.value.strip():\n",
    "        cmd.extend([\"--min-free-bytes\", min_free_input.value.strip()])\n",
    "    for run_id in runs_input.value.splitlines():\n",
    "        run_id = run_id.strip()\n",
    "        if run_id:\n",
    "            cmd.extend([\"--run\", run_id])\n",
    "    cmd.append(\"--no-dry-run\")\n",
    "    if assume_yes_checkbox.value:\n",
    "        cmd.append(\"--yes\")\n",
    "    return cmd\n",
    "\n",
    "\n",
    "def _run_prune(_btn: widgets.Button) -> None:\n",
    "    log_output.clear_output()\n",
    "    errors = []\n",
    "    effective_backend = (backend_value or os.environ.get(\"ARTIFACTS_BACKEND\", \"\")).lower()\n",
    "    if effective_backend != \"filesystem\":\n",
    "        errors.append(\"Set ARTIFACTS_BACKEND=filesystem before pruning.\")\n",
    "    if not CLI_PATH.exists():\n",
    "        errors.append(f\"Missing CLI script: {CLI_PATH}\")\n",
    "    if not (max_bytes_input.value.strip() or max_age_input.value > 0 or min_free_input.value.strip()):\n",
    "        errors.append(\"Provide at least one retention constraint (max bytes / age / min free).\")\n",
    "    if errors:\n",
    "        status_label.value = _format_status(\" \".join(errors), ok=False)\n",
    "        return\n",
    "\n",
    "    cmd = _build_command()\n",
    "    env = os.environ.copy()\n",
    "    pythonpath_bits = [str(REPO_ROOT), str(REPO_ROOT / \"src\")]\n",
    "    if env.get(\"PYTHONPATH\"):\n",
    "        pythonpath_bits.append(env[\"PYTHONPATH\"])\n",
    "    env[\"PYTHONPATH\"] = os.pathsep.join(pythonpath_bits)\n",
    "\n",
    "    status_label.value = _format_status(\"Running prune command…\", ok=True)\n",
    "\n",
    "    with log_output:\n",
    "        print(\"$\", \" \".join(shlex.quote(part) for part in cmd))\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, env=env)\n",
    "        assert process.stdout is not None\n",
    "        for line in process.stdout:\n",
    "            print(line.rstrip())\n",
    "        return_code = process.wait()\n",
    "\n",
    "    if return_code == 0:\n",
    "        status_label.value = _format_status(\"Prune command completed.\", ok=True)\n",
    "    else:\n",
    "        status_label.value = _format_status(f\"Prune command failed (exit {return_code}).\", ok=False)\n",
    "\n",
    "\n",
    "run_button.on_click(_run_prune)\n",
    "\n",
    "prune_controls = widgets.VBox(\n",
    "    [\n",
    "        widgets.HBox([max_bytes_input, max_age_input, min_free_input]),\n",
    "        runs_input,\n",
    "        widgets.HBox([assume_yes_checkbox, run_button]),\n",
    "        status_label,\n",
    "        log_output,\n",
    "    ]\n",
    ")\n",
    "\n",
    "if ROOT_VALUE and INDEX_VALUE:\n",
    "    status_label.value = _format_status(\"Ready: current workspace paths loaded from env.\", ok=True)\n",
    "\n",
    "display(prune_controls)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkle_motion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
