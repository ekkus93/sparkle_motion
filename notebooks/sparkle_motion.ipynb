{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1f9590f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "REPO_ROOT = Path('/home/phil/work/sparkle_motion')\n",
        "os.chdir(REPO_ROOT)\n",
        "os.environ[\"RUN_ID\"] = \"run_c302c0ce1b30\"\n",
        "os.environ[\"FINAL_VIDEO_DIR\"] = str(REPO_ROOT / \"tmp\" / \"final_videos\")\n",
        "Path(os.environ[\"FINAL_VIDEO_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
        "print(\"CWD:\", Path.cwd())\n",
        "print(\"RUN_ID set to\", os.environ[\"RUN_ID\"])\n",
        "print(\"FINAL_VIDEO_DIR set to\", os.environ[\"FINAL_VIDEO_DIR\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5eda5d3",
      "metadata": {},
      "source": [
        "# sparkle_motion — Colab A100 runbook\n",
        "This notebook **only** supports the Google Colab A100 runtime. Every cell assumes you are in `/content` with a GPU attached.\n",
        "- Step through the cells in order; do not skip the install step.\n",
        "- Choose whether to work out of Google Drive (persistent) or `/content` (ephemeral) by toggling the workspace config cell.\n",
        "- The smoke-test cell is a safe stub that checks imports and shows how to run the orchestrator in simulation mode once the stack is installed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c497420",
      "metadata": {},
      "source": [
        "## Notes and expectations\n",
        "- Always request an A100 runtime in Colab (`Runtime → Change runtime type → GPU → A100`).\n",
        "- Keep the notebook tab in focus while installs run; cancel/restart the runtime if `pip install` fails.\n",
        "- The only decision point is whether to mount Google Drive (recommended) or stay in `/content` for a throwaway run."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f21793c8",
      "metadata": {},
      "source": [
        "## Pull the repository into this runtime\n",
        "Use the next cell when you open this notebook directly from GitHub in Colab. It clones (or updates) the `sparkle_motion` repo inside `/content` so every other helper has access to the full source tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94142526",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell -1: Clone or update the sparkle_motion repo (Colab-friendly)\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "REPO_URL = \"https://github.com/ekkus93/sparkle_motion.git\"\n",
        "TARGET_DIR = Path(\"/content/sparkle_motion\").resolve()\n",
        "\n",
        "\n",
        "def _existing_repo_root(start: Path) -> Optional[Path]:\n",
        "    for candidate in [start, *start.parents]:\n",
        "        if (candidate / \"pyproject.toml\").exists():\n",
        "            return candidate\n",
        "    return None\n",
        "\n",
        "\n",
        "def _run_git(*args: str) -> None:\n",
        "    subprocess.check_call([\"git\", *args])\n",
        "\n",
        "\n",
        "current_dir = Path.cwd().resolve()\n",
        "repo_root = _existing_repo_root(current_dir)\n",
        "\n",
        "if repo_root:\n",
        "    print(f\"Found existing sparkle_motion repo at {repo_root}\")\n",
        "    os.chdir(repo_root)\n",
        "else:\n",
        "    if TARGET_DIR.exists():\n",
        "        if (TARGET_DIR / \".git\").exists():\n",
        "            print(f\"Repository already present at {TARGET_DIR}; pulling latest changes...\")\n",
        "            _run_git(\"-C\", str(TARGET_DIR), \"fetch\", \"--all\", \"--prune\")\n",
        "            _run_git(\"-C\", str(TARGET_DIR), \"pull\", \"--ff-only\")\n",
        "        else:\n",
        "            raise RuntimeError(\n",
        "                f\"{TARGET_DIR} exists but is not a git repository. Delete it or update TARGET_DIR.\"\n",
        "            )\n",
        "    else:\n",
        "        print(f\"Cloning {REPO_URL} into {TARGET_DIR} ...\")\n",
        "        _run_git(\"clone\", REPO_URL, str(TARGET_DIR))\n",
        "    os.chdir(TARGET_DIR)\n",
        "    repo_root = TARGET_DIR\n",
        "\n",
        "REPO_ROOT = Path.cwd().resolve()\n",
        "globals()[\"REPO_ROOT\"] = REPO_ROOT\n",
        "os.environ[\"SPARKLE_MOTION_REPO_ROOT\"] = str(REPO_ROOT)\n",
        "print(f\"Working directory switched to {REPO_ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a393ed89",
      "metadata": {},
      "source": [
        "## Install required ML stack\n",
        "Run this immediately after cloning. It installs every dependency listed in `requirements-ml.txt` and is required for all later cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c67a2d8d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 0: Install everything from requirements-ml.txt (mandatory)\n",
        "import importlib.util\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "if importlib.util.find_spec(\"google.colab\") is None:\n",
        "    raise RuntimeError(\"This notebook only runs on Google Colab with an A100 GPU attached.\")\n",
        "\n",
        "REPO_ROOT = Path(os.environ.get(\"SPARKLE_MOTION_REPO_ROOT\", Path.cwd())).resolve()\n",
        "globals()[\"REPO_ROOT\"] = REPO_ROOT\n",
        "req_path = REPO_ROOT / \"requirements-ml.txt\"\n",
        "if not req_path.exists():\n",
        "    raise FileNotFoundError(f\"requirements-ml.txt not found at {req_path}\")\n",
        "\n",
        "print(\"Installing Python dependencies (this may take several minutes)...\")\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_path)])\n",
        "print(\"Dependency install complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c56db335",
      "metadata": {},
      "source": [
        "## Configure workspace inputs\n",
        "Set the flags below **once**. `USE_DRIVE = True` keeps your assets in Google Drive; setting it to `False` keeps everything under `/content` for a throwaway run. Provide one or more repo IDs via `HF_MODELS`; set `DRY_RUN = True` or leave the list empty to skip downloads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2587a077",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Workspace configuration (choose Drive vs /content)\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "USE_DRIVE = True                         # True = mount Google Drive, False = keep everything under /content\n",
        "WORKSPACE_NAME = \"SparkleMotion\"          # Folder created under MyDrive/ when USE_DRIVE is True\n",
        "HF_MODELS = [\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        " ]\n",
        "DRY_RUN = False\n",
        "MOUNT_POINT = Path(\"/content/drive\")\n",
        "DRIVE_ROOT = MOUNT_POINT / \"MyDrive\" / WORKSPACE_NAME\n",
        "LOCAL_ROOT = Path(\"/content\") / WORKSPACE_NAME\n",
        "\n",
        "REPO_ROOT = Path(os.environ.get(\"SPARKLE_MOTION_REPO_ROOT\", Path.cwd())).resolve()\n",
        "globals()[\"REPO_ROOT\"] = REPO_ROOT\n",
        "\n",
        "workspace_root = DRIVE_ROOT if USE_DRIVE else LOCAL_ROOT\n",
        "print(f\"Configured workspace '{WORKSPACE_NAME}' → {workspace_root}\")\n",
        "print(f\"Google Drive enabled: {USE_DRIVE}\")\n",
        "print(f\"Models to manage: {HF_MODELS or '[none specified]'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e641fa1",
      "metadata": {},
      "source": [
        "## Load secrets from `.env`\n",
        "Keep a `.env` file in your Drive workspace (or `/content`) with the required values **before** running the next cell. The filesystem ArtifactService ships with this repo, so no external cloud credential setup is required—everything stays on the Colab VM or your mounted Drive.\n",
        "- `ADK_PROJECT` — the ADK project slug (usually `sparkle-motion`).\n",
        "- `ADK_API_KEY` — API credential for the ADK control plane. Follow the steps below to create it if you do not already have one.\n",
        "- `ADK_USE_FIXTURE` — set to `0` to talk to the **real ADK control plane** (the hosted service your team operates). Set to `1` for the **local fixture mode**, which uses canned responses for dry runs and never leaves Colab.\n",
        "- `ADK_PUBLISH_INTEGRATION` — leave unset unless you plan to run the ADK publish integration tests (set it to `1` only when running those pytest cases so they get collected).\n",
        "\n",
        "Optional (set them when you know you need the behavior):\n",
        "- `PRODUCTION_AGENT_BASE` / `SCRIPT_AGENT_BASE` — override the default `http://127.0.0.1:{8200,8101}` endpoints if your agents run elsewhere.\n",
        "- `FINAL_VIDEO_DIR` — explicit folder for final MP4 downloads; defaults to the workspace’s `final_videos/`.\n",
        "- GPU toggles such as `SMOKE_IMAGES`, `SMOKE_VIDEOS`, `IMAGES_SDXL_FIXTURE_ONLY`, `VIDEOS_WAN_FIXTURE_ONLY` when you want to exercise real adapters instead of fixtures (set them to `0` to force the real pipelines).\n",
        "\n",
        "**How to create an `ADK_API_KEY` (all inside this notebook workflow):**\n",
        "1. Make sure you can reach the ADK control plane for the `sparkle-motion` project. If you have not logged in yet, run `adk auth login --project sparkle-motion` (from Cloud Shell or your laptop) and finish the browser login it launches, or open the ADK console URL you deployed earlier and select the project there.\n",
        "2. Sign in via either path: (a) open the ADK console URL in a browser and select the `sparkle-motion` project, or (b) run the CLI command above and finish the browser login it launches.\n",
        "3. Create a key:\n",
        "   - Console: go to **Projects → sparkle-motion → Security → API keys → Create key**, give it a name such as \"colab-notebook\", and check the scopes `artifacts.read` + `artifacts.write`.\n",
        "   - CLI: run `adk keys create --project sparkle-motion --display-name colab-notebook --scopes artifacts.read artifacts.write`.\n",
        "4. Copy the `apiKey` value (it starts with `sk-`) the moment it is shown and paste it into your `.env` in this Colab workspace:\n",
        "   ```\n",
        "   ADK_PROJECT=sparkle-motion\n",
        "   ADK_API_KEY=sk-live-xxxxxxxx\n",
        "   ```\n",
        "   Keep this file out of version control—treat the key like any other secret.\n",
        "5. Rerun the next cell so `python-dotenv` loads the updated `.env` into the Colab kernel.\n",
        "6. Whenever you rotate the value, overwrite the entry in `.env` and rerun the loader cell; no other files need to change.\n",
        "\n",
        "Once the file is populated, run the next cell; it installs `python-dotenv` if needed and loads every variable into the current Colab kernel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdb81eb0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1b: Load secrets from .env using python-dotenv\n",
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def _ensure_python_dotenv_installed() -> None:\n",
        "    if importlib.util.find_spec(\"dotenv\") is None:\n",
        "        print(\"Installing python-dotenv...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"python-dotenv\"])\n",
        "\n",
        "\n",
        "_ensure_python_dotenv_installed()\n",
        "from dotenv import load_dotenv  # type: ignore  # imported after ensuring package\n",
        "\n",
        "workspace_root = DRIVE_ROOT if USE_DRIVE else LOCAL_ROOT\n",
        "candidate_paths = [\n",
        "    REPO_ROOT / \".env.local\",\n",
        "    REPO_ROOT / \".env\",\n",
        "    Path(\"/content/.env\"),\n",
        "    workspace_root / \".env\",\n",
        "]\n",
        "\n",
        "env_path = next((path for path in candidate_paths if path.exists()), None)\n",
        "if env_path is None:\n",
        "    print(\n",
        "        \"No .env file found. Run scripts/bootstrap_adk_projects.sh --profile local-colab \"\n",
        "        \"from the repo root, copy the file into your workspace, and re-run this cell.\",\n",
        "    )\n",
        "else:\n",
        "    load_dotenv(env_path, override=True)\n",
        "    print(f\"Loaded environment variables from {env_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6246302d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Mount Google Drive when USE_DRIVE is True (Colab-only)\n",
        "import importlib.util\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "if importlib.util.find_spec(\"google.colab\") is None:\n",
        "    raise RuntimeError(\"This notebook must run inside Google Colab.\")\n",
        "\n",
        "if not USE_DRIVE:\n",
        "    workspace_root = LOCAL_ROOT\n",
        "    workspace_root.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"USE_DRIVE is False — using ephemeral workspace at {workspace_root}\")\n",
        "else:\n",
        "    from google.colab import drive\n",
        "\n",
        "    mount_target = Path(MOUNT_POINT)\n",
        "    mount_target.mkdir(parents=True, exist_ok=True)\n",
        "    if os.path.ismount(mount_target):\n",
        "        print(f\"Google Drive already mounted at {mount_target}.\")\n",
        "    else:\n",
        "        print(f\"Mounting Google Drive at {mount_target}...\")\n",
        "        drive.mount(str(mount_target), force_remount=False)\n",
        "\n",
        "    workspace_root = DRIVE_ROOT\n",
        "    workspace_root.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Workspace directory ready at {workspace_root}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e6a038a",
      "metadata": {},
      "source": [
        "## Colab preflight helper\n",
        "Use this helper right after the install cell to confirm ADC auth, env vars, Drive mount, and `/ready` endpoints before touching the control panel. The helper wraps `python -m sparkle_motion.notebook_preflight` so you can rerun it any time during the session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34d1cd8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2a: Run the consolidated Colab preflight checks\n",
        "from pathlib import Path\n",
        "from sparkle_motion.notebook_preflight import format_report, run_preflight_checks\n",
        "\n",
        "workspace_root = DRIVE_ROOT if USE_DRIVE else LOCAL_ROOT\n",
        "\n",
        "preflight_results = run_preflight_checks(\n",
        "    requirements_path=REPO_ROOT / \"requirements-ml.txt\",\n",
        "    mount_point=MOUNT_POINT,\n",
        "    workspace_dir=workspace_root,\n",
        "    ready_endpoints=(\n",
        "        \"http://localhost:8101/ready\",\n",
        "        \"http://localhost:8200/ready\",\n",
        "    ),\n",
        "    pip_mode=\"install\",\n",
        "    require_drive=USE_DRIVE,\n",
        "    skip_gpu_checks=False,\n",
        ")\n",
        "\n",
        "print(format_report(preflight_results))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b720abb",
      "metadata": {},
      "source": [
        "## Prepare workspace and download models\n",
        "Use `scripts/colab_drive_setup.py` to create the Drive folders (or `/content` folders when `USE_DRIVE = False`), optionally download Hugging Face weights, and write a smoke artifact in `outputs/colab_smoke.json`.\n",
        "- Leave `HF_MODELS` empty for a dry run, or list specific repo IDs to download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4d0ccee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Prepare workspace directories and download models\n",
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "if importlib.util.find_spec(\"google.colab\") is None:\n",
        "    raise RuntimeError(\"Run this helper inside Google Colab.\")\n",
        "\n",
        "helper_path = REPO_ROOT / \"scripts\" / \"colab_drive_setup.py\"\n",
        "if not helper_path.exists():\n",
        "    raise FileNotFoundError(f\"Helper script not found at {helper_path}\")\n",
        "\n",
        "cmd = [sys.executable, str(helper_path), WORKSPACE_NAME]\n",
        "if USE_DRIVE:\n",
        "    cmd.extend([\"--mount-point\", str(MOUNT_POINT)])\n",
        "else:\n",
        "    workspace_root = LOCAL_ROOT\n",
        "    workspace_root.mkdir(parents=True, exist_ok=True)\n",
        "    cmd.extend([\"--local-root\", str(workspace_root)])\n",
        "for repo_id in HF_MODELS:\n",
        "    cmd.extend([\"--model\", repo_id])\n",
        "if DRY_RUN:\n",
        "    cmd.append(\"--dry-run\")\n",
        "\n",
        "print(\"Running helper:\", \" \".join(map(str, cmd)))\n",
        "subprocess.check_call(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb8e69ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3b: Inspect smoke artifact with per-model status\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "workspace_root = DRIVE_ROOT if USE_DRIVE else LOCAL_ROOT\n",
        "smoke_path = workspace_root / \"outputs\" / \"colab_smoke.json\"\n",
        "if smoke_path.exists():\n",
        "    data = json.loads(smoke_path.read_text(encoding=\"utf-8\"))\n",
        "    status = \"OK\" if data.get(\"ok\") else \"FAILED\"\n",
        "    print(f\"Smoke status: {status}\")\n",
        "    for model in data.get(\"models\", []):\n",
        "        sample = model.get(\"sample_file\") or \"n/a\"\n",
        "        print(\n",
        "            f\"- {model['repo_id']}: {model['status']} \"\n",
        "            f\"({model.get('files_present', 0)} files, sample={sample})\"\n",
        ")\n",
        "else:\n",
        "    print(f\"No smoke artifact found at {smoke_path}. Run the helper once to generate it.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9c97200",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Smoke-test stub for the orchestrator (safe, non-destructive)\n",
        "# This cell attempts to import the orchestrator package and reports what is available.\n",
        "try:\n",
        "    import sparkle_motion.orchestrator as orchestrator_mod\n",
        "    print('Imported sparkle_motion.orchestrator ->', orchestrator_mod)\n",
        "    if hasattr(orchestrator_mod, 'Runner'):\n",
        "        print('Runner class is available. Instantiate it for a simulation run:')\n",
        "        print(\"  from sparkle_motion.orchestrator import Runner\")\n",
        "        print(\"  r = Runner(run_dir='runs')\")\n",
        "        print(\"  # then use r.run(...) or similar per your orchestrator API\")\n",
        "    else:\n",
        "        print('Runner class not found — inspect src/sparkle_motion/orchestrator.py for usage.')\n",
        "except Exception as e:\n",
        "    print('Could not import orchestrator module:', e)\n",
        "    print('Ensure the package is installed (pip install -e .) and rerun this cell.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16a9c968",
      "metadata": {},
      "source": [
        "## Workflow Agent server controls\n",
        "These helpers let you start and stop the local Workflow Agent FastAPI apps (script and production agents) directly from the notebook. They simply launch `uvicorn` in the background, track each process (`pid`), and make it easy to shut them down again when you finish. Run them only when you are *not* already running the servers in another terminal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5b22185",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import signal\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Dict, NamedTuple\n",
        "\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display\n",
        "except ImportError as exc:  # pragma: no cover - notebook utility guard\n",
        "    raise RuntimeError(\"ipywidgets is required for the Workflow Agent controls\") from exc\n",
        "\n",
        "\n",
        "class ServerProcess(NamedTuple):\n",
        "    process: subprocess.Popen\n",
        "    log_path: Path\n",
        "    log_handle: object\n",
        "\n",
        "\n",
        "SERVER_CONFIGS: Dict[str, Dict[str, object]] = {\n",
        "    \"script_agent\": {\n",
        "        \"app\": \"sparkle_motion.function_tools.script_agent.entrypoint:app\",\n",
        "        \"port\": 8101,\n",
        "    },\n",
        "    \"production_agent\": {\n",
        "        \"app\": \"sparkle_motion.function_tools.production_agent.entrypoint:app\",\n",
        "        \"port\": 8200,\n",
        "    },\n",
        "}\n",
        "\n",
        "RUNNING_SERVERS: Dict[str, ServerProcess] = {}\n",
        "\n",
        "PYTHONPATH = os.pathsep.join(\n",
        "    filter(None, {str(Path(REPO_ROOT)), str(Path(REPO_ROOT) / \"src\"), os.environ.get(\"PYTHONPATH\", \"\")})\n",
        ")\n",
        "\n",
        "\n",
        "def _server_cmd(name: str) -> list[str]:\n",
        "    cfg = SERVER_CONFIGS[name]\n",
        "    return [\n",
        "        sys.executable,\n",
        "        \"-m\",\n",
        "        \"uvicorn\",\n",
        "        cfg[\"app\"],\n",
        "        \"--host\",\n",
        "        os.environ.get(\"WORKFLOW_AGENT_HOST\", \"127.0.0.1\"),\n",
        "        \"--port\",\n",
        "        str(cfg[\"port\"]),\n",
        "        \"--no-access-log\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def start_server(name: str) -> None:\n",
        "    server = RUNNING_SERVERS.get(name)\n",
        "    if server and server.process.poll() is None:\n",
        "        raise RuntimeError(f\"{name} already running (pid={server.process.pid})\")\n",
        "    env = os.environ.copy()\n",
        "    env[\"PYTHONPATH\"] = PYTHONPATH\n",
        "    log_path = Path(REPO_ROOT) / \"tmp\" / f\"{name}.log\"\n",
        "    log_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    log_handle = open(log_path, \"ab\", buffering=0)\n",
        "    proc = subprocess.Popen(\n",
        "        _server_cmd(name),\n",
        "        env=env,\n",
        "        stdout=log_handle,\n",
        "        stderr=subprocess.STDOUT,\n",
        "    )\n",
        "    RUNNING_SERVERS[name] = ServerProcess(process=proc, log_path=log_path, log_handle=log_handle)\n",
        "    print(f\"Started {name} on port {SERVER_CONFIGS[name]['port']} (pid={proc.pid}). Logs → {log_path}\")\n",
        "\n",
        "\n",
        "def stop_server(name: str, *, sig: int = signal.SIGTERM) -> None:\n",
        "    server = RUNNING_SERVERS.get(name)\n",
        "    if not server:\n",
        "        print(f\"{name} has not been started from this notebook.\")\n",
        "        return\n",
        "    proc = server.process\n",
        "    if proc.poll() is not None:\n",
        "        print(f\"{name} already stopped (pid={proc.pid}).\")\n",
        "    else:\n",
        "        proc.send_signal(sig)\n",
        "        try:\n",
        "            proc.wait(timeout=10)\n",
        "        except subprocess.TimeoutExpired:\n",
        "            proc.kill()\n",
        "            proc.wait()\n",
        "        print(f\"Stopped {name} (pid={proc.pid}).\")\n",
        "    server.log_handle.close()\n",
        "    RUNNING_SERVERS.pop(name, None)\n",
        "\n",
        "\n",
        "def list_servers() -> Dict[str, str]:\n",
        "    status = {}\n",
        "    for name in SERVER_CONFIGS:\n",
        "        proc = RUNNING_SERVERS.get(name)\n",
        "        if proc and proc.process.poll() is None:\n",
        "            status[name] = f\"running (pid={proc.process.pid})\"\n",
        "        else:\n",
        "            status[name] = \"stopped\"\n",
        "    return status\n",
        "\n",
        "\n",
        "server_selector = widgets.Dropdown(options=list(SERVER_CONFIGS.keys()), description=\"Server\")\n",
        "start_button = widgets.Button(description=\"Start\", button_style=\"success\")\n",
        "stop_button = widgets.Button(description=\"Stop\", button_style=\"danger\")\n",
        "status_button = widgets.Button(description=\"Status\", button_style=\"info\")\n",
        "output = widgets.Output()\n",
        "\n",
        "\n",
        "def _handle_start(_: widgets.Button) -> None:\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        try:\n",
        "            start_server(server_selector.value)\n",
        "        except Exception as exc:  # pragma: no cover - notebook UX guard\n",
        "            print(f\"Failed to start {server_selector.value}: {exc}\")\n",
        "\n",
        "\n",
        "def _handle_stop(_: widgets.Button) -> None:\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        stop_server(server_selector.value)\n",
        "\n",
        "\n",
        "def _handle_status(_: widgets.Button) -> None:\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        for name, state in list_servers().items():\n",
        "            print(f\"{name}: {state}\")\n",
        "\n",
        "\n",
        "start_button.on_click(_handle_start)\n",
        "stop_button.on_click(_handle_stop)\n",
        "status_button.on_click(_handle_status)\n",
        "\n",
        "controls = widgets.HBox([server_selector, start_button, stop_button, status_button])\n",
        "display(widgets.VBox([controls, output]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "909e89ba",
      "metadata": {},
      "source": [
        "## Filesystem ArtifactService shim controls\n",
        "Use these helpers to export the required environment variables, launch the local ArtifactService shim, and verify `/healthz` before switching the notebook to `ARTIFACTS_BACKEND=filesystem`. The buttons below wrap `scripts/filesystem_artifacts.py env|serve|health`, so they stay consistent with the new CLI workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd26b9c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import signal\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Dict\n",
        "\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display\n",
        "except ImportError as exc:  # pragma: no cover - notebook helper\n",
        "    raise RuntimeError(\"ipywidgets is required for the filesystem shim controls\") from exc\n",
        "\n",
        "REPO_ROOT = Path(os.environ.get(\"SPARKLE_MOTION_REPO_ROOT\", Path.cwd())).resolve()\n",
        "SRC_PATH = REPO_ROOT / \"src\"\n",
        "FS_SHIM_HOST = os.environ.get(\"FILESYSTEM_SHIM_HOST\", \"127.0.0.1\")\n",
        "FS_SHIM_PORT = int(os.environ.get(\"FILESYSTEM_SHIM_PORT\", \"7077\"))\n",
        "FS_ROOT = (DRIVE_ROOT if USE_DRIVE else LOCAL_ROOT) / \"artifacts_fs\"\n",
        "FS_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "_shim_process: subprocess.Popen | None = None\n",
        "_shim_log_handle = None\n",
        "_shim_log_path = REPO_ROOT / \"tmp\" / \"filesystem_shim.log\"\n",
        "_shim_log_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _shim_env() -> Dict[str, str]:\n",
        "    env = os.environ.copy()\n",
        "    env.setdefault(\"ARTIFACTS_BACKEND\", \"filesystem\")\n",
        "    env.setdefault(\"ARTIFACTS_FS_ROOT\", str(FS_ROOT))\n",
        "    env.setdefault(\"ARTIFACTS_FS_INDEX\", str(FS_ROOT / \"index.db\"))\n",
        "    env.setdefault(\"ARTIFACTS_FS_BASE_URL\", f\"http://{FS_SHIM_HOST}:{FS_SHIM_PORT}\")\n",
        "    env.setdefault(\"ARTIFACTS_FS_TOKEN\", env.get(\"ARTIFACTS_FS_TOKEN\") or \"local-fs-token\")\n",
        "    env[\"PYTHONPATH\"] = os.pathsep.join(filter(None, {str(REPO_ROOT), str(SRC_PATH), env.get(\"PYTHONPATH\", \"\")}))\n",
        "    return env\n",
        "\n",
        "def _start_filesystem_shim(_: widgets.Button | None = None) -> None:\n",
        "    global _shim_process, _shim_log_handle\n",
        "    if _shim_process and _shim_process.poll() is None:\n",
        "        with shim_output:\n",
        "            shim_output.clear_output()\n",
        "            print(f\"Filesystem shim already running on {FS_SHIM_HOST}:{FS_SHIM_PORT} (pid={_shim_process.pid}).\")\n",
        "        return\n",
        "    env = _shim_env()\n",
        "    _shim_log_handle = open(_shim_log_path, \"ab\", buffering=0)\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        \"scripts/filesystem_artifacts.py\",\n",
        "        \"serve\",\n",
        "        \"--host\",\n",
        "        FS_SHIM_HOST,\n",
        "        \"--port\",\n",
        "        str(FS_SHIM_PORT),\n",
        "    ]\n",
        "    _shim_process = subprocess.Popen(cmd, env=env, stdout=_shim_log_handle, stderr=subprocess.STDOUT)\n",
        "    with shim_output:\n",
        "        shim_output.clear_output()\n",
        "        print(f\"Started filesystem shim on {FS_SHIM_HOST}:{FS_SHIM_PORT} (pid={_shim_process.pid}).\")\n",
        "        print(f\"Logs → {_shim_log_path}\")\n",
        "\n",
        "def _stop_filesystem_shim(_: widgets.Button | None = None) -> None:\n",
        "    global _shim_process, _shim_log_handle\n",
        "    if not _shim_process:\n",
        "        with shim_output:\n",
        "            shim_output.clear_output()\n",
        "            print(\"Filesystem shim has not been started from this notebook.\")\n",
        "        return\n",
        "    if _shim_process.poll() is None:\n",
        "        _shim_process.send_signal(signal.SIGTERM)\n",
        "        try:\n",
        "            _shim_process.wait(timeout=10)\n",
        "        except subprocess.TimeoutExpired:\n",
        "            _shim_process.kill()\n",
        "            _shim_process.wait()\n",
        "    if _shim_log_handle:\n",
        "        _shim_log_handle.close()\n",
        "        _shim_log_handle = None\n",
        "    with shim_output:\n",
        "        shim_output.clear_output()\n",
        "        print(\"Filesystem shim stopped.\")\n",
        "    _shim_process = None\n",
        "\n",
        "def _shim_status(_: widgets.Button | None = None) -> None:\n",
        "    with shim_output:\n",
        "        shim_output.clear_output()\n",
        "        if _shim_process and _shim_process.poll() is None:\n",
        "            print(f\"Running on {FS_SHIM_HOST}:{FS_SHIM_PORT} (pid={_shim_process.pid}).\")\n",
        "        else:\n",
        "            print(\"Filesystem shim is stopped.\")\n",
        "\n",
        "def _print_env_exports(_: widgets.Button | None = None) -> None:\n",
        "    env = _shim_env()\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        \"scripts/filesystem_artifacts.py\",\n",
        "        \"env\",\n",
        "        \"--shell\",\n",
        "        \"bash\",\n",
        "        \"--root\",\n",
        "        env[\"ARTIFACTS_FS_ROOT\"],\n",
        "        \"--index\",\n",
        "        env[\"ARTIFACTS_FS_INDEX\"],\n",
        "        \"--token\",\n",
        "        env[\"ARTIFACTS_FS_TOKEN\"],\n",
        "        \"--emit-token\",\n",
        "    ]\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True, env=env)\n",
        "    with env_output:\n",
        "        env_output.clear_output()\n",
        "        if result.returncode == 0:\n",
        "            print(result.stdout.strip())\n",
        "        else:\n",
        "            print(result.stderr or result.stdout)\n",
        "\n",
        "def _health_probe(_: widgets.Button | None = None) -> None:\n",
        "    env = _shim_env()\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        \"scripts/filesystem_artifacts.py\",\n",
        "        \"health\",\n",
        "        \"--url\",\n",
        "        f\"http://{FS_SHIM_HOST}:{FS_SHIM_PORT}/healthz\",\n",
        "        \"--token\",\n",
        "        env[\"ARTIFACTS_FS_TOKEN\"],\n",
        "    ]\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True, env=env)\n",
        "    with env_output:\n",
        "        env_output.clear_output()\n",
        "        if result.returncode == 0:\n",
        "            print(result.stdout.strip())\n",
        "        else:\n",
        "            print(result.stderr or result.stdout)\n",
        "\n",
        "start_button = widgets.Button(description=\"Start shim\", button_style=\"success\")\n",
        "stop_button = widgets.Button(description=\"Stop shim\", button_style=\"danger\")\n",
        "status_button = widgets.Button(description=\"Status\", button_style=\"info\")\n",
        "env_button = widgets.Button(description=\"Show env exports\")\n",
        "health_button = widgets.Button(description=\"Probe /healthz\", icon=\"heartbeat\")\n",
        "shim_output = widgets.Output()\n",
        "env_output = widgets.Output()\n",
        "\n",
        "start_button.on_click(_start_filesystem_shim)\n",
        "stop_button.on_click(_stop_filesystem_shim)\n",
        "status_button.on_click(_shim_status)\n",
        "env_button.on_click(_print_env_exports)\n",
        "health_button.on_click(_health_probe)\n",
        "\n",
        "controls_row = widgets.HBox([start_button, stop_button, status_button, env_button, health_button])\n",
        "display(widgets.VBox([controls_row, shim_output, env_output]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08966a60",
      "metadata": {},
      "source": [
        "## Quickstart: Launch the control panel\n",
        "Run this cell after the FunctionTools are live (script_agent + production_agent listening on localhost). It imports `create_control_panel`, instantiates the widgets with the `local-colab` profile, and stores the panel in `control_panel` so later helpers (status polling, final deliverable preview) can reuse the same run metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4da3395",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "\n",
        "REPO_ROOT = Path(os.environ.get(\"SPARKLE_MOTION_REPO_ROOT\", Path.cwd())).resolve()\n",
        "globals()[\"REPO_ROOT\"] = REPO_ROOT\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.append(str(REPO_ROOT))\n",
        "SRC_PATH = REPO_ROOT / \"src\"\n",
        "if str(SRC_PATH) not in sys.path:\n",
        "    sys.path.append(str(SRC_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1289a2b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quickstart cell: import and display the ipywidgets control panel\n",
        "from notebooks.control_panel import create_control_panel\n",
        "\n",
        "print(\"Launching control panel with endpoints from configs/tool_registry.yaml (profile='local-colab').\")\n",
        "control_panel = create_control_panel()\n",
        "control_panel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "824ad861",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper: set run_id for artifacts viewing/tests\n",
        "TARGET_RUN_ID = \"run_a12af6a94ab5\"  # Latest local run with finalize artifacts\n",
        "if \"control_panel\" in globals():\n",
        "    if hasattr(control_panel, \"run_id_input\"):\n",
        "        control_panel.run_id_input.value = TARGET_RUN_ID\n",
        "    state = getattr(control_panel, \"state\", None)\n",
        "    if state is not None:\n",
        "        state.last_run_id = TARGET_RUN_ID\n",
        "    print(f\"Control panel run_id set to {TARGET_RUN_ID}\")\n",
        "else:\n",
        "    print(\"control_panel is not initialized; run the quickstart cell first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adccde6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch a fresh production run for notebook verification\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "import httpx\n",
        "\n",
        "PLAN_PATH = (REPO_ROOT / \"artifacts\" / \"Test_Film.json\").resolve()\n",
        "if not PLAN_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Sample plan not found at {PLAN_PATH}\")\n",
        "plan_payload_raw = json.loads(PLAN_PATH.read_text())\n",
        "plan_payload = plan_payload_raw.get(\"validated_plan\") or plan_payload_raw\n",
        "request_body = {\"mode\": \"run\", \"plan\": plan_payload}\n",
        "with httpx.Client(timeout=60.0) as client:\n",
        "    invoke_resp = client.post(f\"{PRODUCTION_AGENT_BASE}/invoke\", json=request_body)\n",
        "    if invoke_resp.status_code >= 400:\n",
        "        snippet = invoke_resp.text[:800]\n",
        "        print(\"Invoke failed (truncated):\", snippet)\n",
        "        invoke_resp.raise_for_status()\n",
        "    invoke_data = invoke_resp.json()\n",
        "run_id_new = invoke_data[\"run_id\"]\n",
        "print(\"Production run_id:\", run_id_new)\n",
        "\n",
        "def _poll_status(run_id: str, *, timeout_s: float = 40.0) -> None:\n",
        "    with httpx.Client(timeout=10.0) as client:\n",
        "        start = time.time()\n",
        "        attempt = 0\n",
        "        while time.time() - start < timeout_s:\n",
        "            status = client.get(f\"{PRODUCTION_AGENT_BASE}/status\", params={\"run_id\": run_id}).json()\n",
        "            print(\n",
        "            f\"poll {attempt}: status={status.get('status')} steps={len(status.get('steps', []))}\"\n",
        "            )\n",
        "            if status.get(\"status\") == \"succeeded\":\n",
        "                return\n",
        "            attempt += 1\n",
        "            time.sleep(0.5)\n",
        "        raise RuntimeError(\"Run did not complete within timeout\")\n",
        "\n",
        "_poll_status(run_id_new)\n",
        "if \"control_panel\" in globals():\n",
        "    if hasattr(control_panel, \"run_id_input\"):\n",
        "        control_panel.run_id_input.value = run_id_new\n",
        "    state = getattr(control_panel, \"state\", None)\n",
        "    if state is not None:\n",
        "        state.last_run_id = run_id_new\n",
        "print(\"Control panel updated with new run_id.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8d24cce",
      "metadata": {},
      "source": [
        "## Notebook control panel prototype (advanced)\n",
        "Need to override the timeout, point at a different profile, or inspect the underlying widgets? Use the cell below to instantiate `ControlPanel` manually. It shows how to swap endpoint profiles, tweak HTTP timeouts, and still reuse the global `control_panel` handle for downstream helpers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1275a633",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced control panel prototype: customize endpoints/timeouts\n",
        "from notebooks.control_panel import ControlPanel, PanelEndpoints\n",
        "\n",
        "CUSTOM_PROFILE = \"local-colab\"  # change to another profile defined in configs/tool_registry.yaml\n",
        "CUSTOM_TIMEOUT_S = 45.0\n",
        "\n",
        "print(f\"Building ControlPanel(profile={CUSTOM_PROFILE!r}, timeout={CUSTOM_TIMEOUT_S}s)...\")\n",
        "custom_endpoints = PanelEndpoints.from_registry(profile=CUSTOM_PROFILE)\n",
        "advanced_control_panel = ControlPanel(endpoints=custom_endpoints, http_timeout_s=CUSTOM_TIMEOUT_S)\n",
        "\n",
        "# Keep downstream helpers working by updating the shared reference.\n",
        "control_panel = advanced_control_panel\n",
        "advanced_control_panel.container"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47ab0c57",
      "metadata": {},
      "source": [
        "## Final deliverable preview & download\n",
        "Use this helper after a production run completes. It fetches the `video_final` artifact from `finalize`, embeds it inline, and offers a Google Colab download when available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed4be777",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: set FINAL_VIDEO_DIR based on workspace choice\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "workspace_root = DRIVE_ROOT if USE_DRIVE else LOCAL_ROOT\n",
        "final_dir = workspace_root / \"final_videos\"\n",
        "final_dir.mkdir(parents=True, exist_ok=True)\n",
        "os.environ[\"FINAL_VIDEO_DIR\"] = str(final_dir)\n",
        "print(f\"FINAL_VIDEO_DIR set to {final_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d6c6576",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Final deliverable helper (finalize)\n",
        "import importlib\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "import httpx\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "from notebooks import preview_helpers\n",
        "\n",
        "PRODUCTION_AGENT_BASE = os.environ.get(\"PRODUCTION_AGENT_BASE\", \"http://127.0.0.1:8200\")\n",
        "FINALIZE_STAGE = \"finalize\"\n",
        "DOWNLOAD_DIR = Path(os.environ.get(\"FINAL_VIDEO_DIR\", \"/content/final_videos\"))\n",
        "DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "colab_files = None\n",
        "try:\n",
        "    _colab_spec = importlib.util.find_spec(\"google.colab.files\")\n",
        "except ModuleNotFoundError:\n",
        "    _colab_spec = None\n",
        "if _colab_spec:\n",
        "    colab_files = importlib.import_module(\"google.colab.files\")\n",
        "\n",
        "\n",
        "def _current_run_id() -> str:\n",
        "    if \"control_panel\" in globals():\n",
        "        cp = globals()[\"control_panel\"]\n",
        "        run_value = getattr(getattr(cp, \"run_id_input\", None), \"value\", \"\")\n",
        "        if run_value and run_value.strip():\n",
        "            return run_value.strip()\n",
        "        state = getattr(cp, \"state\", None)\n",
        "        if state and getattr(state, \"last_run_request_id\", None):\n",
        "            return state.last_run_request_id\n",
        "    return os.environ.get(\"RUN_ID\", \"\").strip()\n",
        "\n",
        "\n",
        "RUN_ID = _current_run_id()\n",
        "if not RUN_ID:\n",
        "    raise RuntimeError(\n",
        "        \"Set RUN_ID (or populate control_panel.run_id_input) before running the final deliverable helper.\",\n",
        "    )\n",
        "\n",
        "\n",
        "def _fetch_stage_manifest(run_id: str) -> Dict[str, Any]:\n",
        "    stage_manifest = preview_helpers.fetch_stage_manifest(\n",
        "        base_url=PRODUCTION_AGENT_BASE,\n",
        "        run_id=run_id,\n",
        "        stage=FINALIZE_STAGE,\n",
        "    )\n",
        "    summary = preview_helpers.render_stage_summary(stage_manifest)\n",
        "    print(summary)\n",
        "    finalize_status = stage_manifest.get(\"status\") or stage_manifest.get(\"state\")\n",
        "    if finalize_status:\n",
        "        print(f\"Finalize status => {finalize_status}\")\n",
        "    return stage_manifest\n",
        "\n",
        "\n",
        "def _locate_video_final(stage_manifest: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    for entry in stage_manifest.get(\"artifacts\") or []:\n",
        "        if entry.get(\"artifact_type\") == \"video_final\":\n",
        "            return entry\n",
        "    raise RuntimeError(\n",
        "        \"No video_final artifact found. Ensure finalize succeeded or resume production_agent with resume_from='finalize'.\",\n",
        "    )\n",
        "\n",
        "\n",
        "def ensure_local_video(entry: Dict[str, Any], run_id: str) -> Path:\n",
        "    local_path = entry.get(\"local_path\")\n",
        "    if local_path:\n",
        "        candidate = Path(local_path).expanduser()\n",
        "        if candidate.exists():\n",
        "            return candidate\n",
        "    target = DOWNLOAD_DIR / f\"{run_id}_video_final.mp4\"\n",
        "    download_url = entry.get(\"download_url\")\n",
        "    if download_url:\n",
        "        with httpx.Client(timeout=None) as client:\n",
        "            with client.stream(\"GET\", download_url) as resp:\n",
        "                resp.raise_for_status()\n",
        "                with target.open(\"wb\") as handle:\n",
        "                    for chunk in resp.iter_bytes():\n",
        "                        handle.write(chunk)\n",
        "        return target\n",
        "    artifact_uri = entry.get(\"artifact_uri\")\n",
        "    if artifact_uri:\n",
        "        result = subprocess.run(\n",
        "            [\"adk\", \"artifacts\", \"download\", artifact_uri, str(target)],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=False,\n",
        "        )\n",
        "        if result.returncode == 0 and target.exists():\n",
        "            return target\n",
        "        print(\"ADK artifact download failed:\")\n",
        "        print(result.stderr or result.stdout)\n",
        "    raise RuntimeError(\n",
        "        \"Unable to download the final video locally. Provide download_url or local_path in the artifacts manifest.\",\n",
        "    )\n",
        "\n",
        "\n",
        "def render_preview_video(preview_entry: Optional[Dict[str, Any]]) -> None:\n",
        "    if not preview_entry:\n",
        "        print(\"No preview clip available from /artifacts preview metadata.\")\n",
        "        return\n",
        "    local_path = preview_entry.get(\"local_path\")\n",
        "    if local_path and Path(local_path).exists():\n",
        "        display(HTML(\"<strong>Preview clip</strong>\"))\n",
        "        preview_widget = preview_helpers.create_video_widget(\n",
        "            {\"local_path\": local_path, \"artifact_type\": \"preview_clip\"},\n",
        "            width=480,\n",
        "        )\n",
        "        display(preview_widget)\n",
        "        return\n",
        "    download_url = preview_entry.get(\"download_url\")\n",
        "    if download_url:\n",
        "        print(f\"Preview available remotely (download_url={download_url}).\")\n",
        "    else:\n",
        "        print(\"Preview metadata present but no local path or download URL; see artifact_uri for details.\")\n",
        "\n",
        "\n",
        "stage_manifest = _fetch_stage_manifest(RUN_ID)\n",
        "preview_entry = (stage_manifest.get(\"preview\") or {}).get(\"video\")\n",
        "final_entry = _locate_video_final(stage_manifest)\n",
        "\n",
        "render_preview_video(preview_entry)\n",
        "video_path = ensure_local_video(final_entry, RUN_ID)\n",
        "final_entry[\"local_path\"] = str(video_path)\n",
        "\n",
        "info_html = f\"\"\"\n",
        "<p><strong>Run ID:</strong> {RUN_ID}</p>\n",
        "<p><strong>Artifact URI:</strong> {final_entry.get('artifact_uri', 'n/a')}</p>\n",
        "<p><strong>Local path:</strong> {video_path}</p>\n",
        "\"\"\"\n",
        "display(HTML(info_html))\n",
        "\n",
        "display(preview_helpers.create_video_widget(final_entry, width=640))\n",
        "\n",
        "if colab_files is not None:\n",
        "    colab_files.download(str(video_path))\n",
        "else:\n",
        "    print(\"Download helper available only inside Google Colab. Share the path above manually if running elsewhere.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aa4b638",
      "metadata": {},
      "source": [
        "## Artifacts viewer\n",
        "Use this helper to inspect `/artifacts` responses without leaving the notebook. It shares the `control_panel` run metadata when available, lets you scope by stage (e.g., `finalize`), and can poll automatically so new artifacts appear as production advances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eebb6b88",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4c: Artifacts viewer helper\n",
        "import asyncio\n",
        "import json\n",
        "import os\n",
        "from typing import Any, Dict\n",
        "\n",
        "import httpx\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "from notebooks import preview_helpers\n",
        "\n",
        "PRODUCTION_AGENT_BASE = os.environ.get(\"PRODUCTION_AGENT_BASE\", \"http://127.0.0.1:8200\")\n",
        "ARTIFACTS_ENDPOINT = f\"{PRODUCTION_AGENT_BASE}/artifacts\"\n",
        "\n",
        "if \"artifact_viewer_state\" in globals():\n",
        "    existing_task = globals()[\"artifact_viewer_state\"].get(\"task\")\n",
        "    if existing_task and not existing_task.done():\n",
        "        existing_task.cancel()\n",
        "\n",
        "artifact_viewer_state = {\"task\": None}\n",
        "\n",
        "\n",
        "def _artifact_viewer_run_id() -> str:\n",
        "    if \"control_panel\" in globals():\n",
        "        cp = globals()[\"control_panel\"]\n",
        "        run_widget = getattr(cp, \"run_id_input\", None)\n",
        "        candidate = getattr(run_widget, \"value\", \"\")\n",
        "        if candidate and candidate.strip():\n",
        "            return candidate.strip()\n",
        "        state = getattr(cp, \"state\", None)\n",
        "        if state and getattr(state, \"last_run_request_id\", None):\n",
        "            return state.last_run_request_id\n",
        "    return os.environ.get(\"RUN_ID\", \"\").strip()\n",
        "\n",
        "\n",
        "def _format_status_html(message: str, *, ok: bool) -> str:\n",
        "    color = \"#3c763d\" if ok else \"#d9534f\"\n",
        "    return f\"<span style='color:{color}; font-size:0.9em;'>{message}</span>\"\n",
        "\n",
        "\n",
        "def _iter_artifacts(payload: Dict[str, Any]):\n",
        "    stages = payload.get(\"stages\") or []\n",
        "    for stage in stages:\n",
        "        for artifact in stage.get(\"artifacts\") or []:\n",
        "            yield artifact\n",
        "    for artifact in payload.get(\"artifacts\") or []:\n",
        "        yield artifact\n",
        "\n",
        "\n",
        "def _render_artifacts(payload: Dict[str, Any]) -> None:\n",
        "    artifacts = list(_iter_artifacts(payload))\n",
        "    stages = payload.get(\"stages\") or []\n",
        "    with artifacts_output:\n",
        "        artifacts_output.clear_output()\n",
        "        print(f\"Artifacts returned: {len(artifacts)}\")\n",
        "        if stages:\n",
        "            for stage in stages:\n",
        "                stage_label = stage.get(\"stage\") or stage.get(\"stage_id\") or \"stage\"\n",
        "                print(f\"\\nStage: {stage_label}\")\n",
        "                print(preview_helpers.render_stage_summary(stage))\n",
        "                previewable = [entry for entry in (stage.get(\"artifacts\") or []) if entry.get(\"local_path\")]\n",
        "                if previewable:\n",
        "                    preview_helpers.display_artifact_previews(\n",
        "                        {**stage, \"artifacts\": previewable},\n",
        "                        max_items=4,\n",
        "                        video_width=360,\n",
        "                    )\n",
        "                else:\n",
        "                    print(\"Local previews unavailable yet; see raw payload below.\")\n",
        "        else:\n",
        "            print(\"No stage sections returned; dumping raw payload.\")\n",
        "        print(\"\\nFull payload:\\n\")\n",
        "        print(json.dumps(payload, indent=2, ensure_ascii=False))\n",
        "\n",
        "\n",
        "def _fetch_artifacts_sync(run_id: str, stage: str) -> Dict[str, Any]:\n",
        "    params = {\"run_id\": run_id}\n",
        "    if stage:\n",
        "        params[\"stage\"] = stage\n",
        "    with httpx.Client(timeout=30.0) as client:\n",
        "        resp = client.get(ARTIFACTS_ENDPOINT, params=params)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "        if not isinstance(data, dict):\n",
        "            raise RuntimeError(\"Unexpected artifacts response payload\")\n",
        "        return data\n",
        "\n",
        "\n",
        "async def _fetch_artifacts_async(client: httpx.AsyncClient, run_id: str, stage: str) -> Dict[str, Any]:\n",
        "    params = {\"run_id\": run_id}\n",
        "    if stage:\n",
        "        params[\"stage\"] = stage\n",
        "    resp = await client.get(ARTIFACTS_ENDPOINT, params=params)\n",
        "    resp.raise_for_status()\n",
        "    data = resp.json()\n",
        "    if not isinstance(data, dict):\n",
        "        raise RuntimeError(\"Unexpected artifacts response payload\")\n",
        "    return data\n",
        "\n",
        "\n",
        "def _stop_artifact_poll(*, from_toggle: bool = False) -> None:\n",
        "    task = artifact_viewer_state.get(\"task\")\n",
        "    if task and not task.done():\n",
        "        task.cancel()\n",
        "    artifact_viewer_state[\"task\"] = None\n",
        "    if not from_toggle:\n",
        "        auto_refresh_toggle.value = False\n",
        "\n",
        "\n",
        "def _handle_manual_refresh(_: Any) -> None:\n",
        "    run_id = run_id_input.value.strip() or _artifact_viewer_run_id()\n",
        "    if not run_id:\n",
        "        status_label.value = _format_status_html(\"Set a Run ID to fetch artifacts.\", ok=False)\n",
        "        with artifacts_output:\n",
        "            artifacts_output.clear_output()\n",
        "            print(\"Provide a Run ID before refreshing artifacts.\")\n",
        "        return\n",
        "    try:\n",
        "        payload = _fetch_artifacts_sync(run_id, stage_input.value.strip())\n",
        "    except httpx.HTTPError as exc:\n",
        "        status_label.value = _format_status_html(f\"Fetch failed: {exc}\", ok=False)\n",
        "        return\n",
        "    except Exception as exc:\n",
        "        status_label.value = _format_status_html(f\"Unexpected error: {exc}\", ok=False)\n",
        "        return\n",
        "    _render_artifacts(payload)\n",
        "    status_label.value = _format_status_html(\"Artifacts refreshed.\", ok=True)\n",
        "\n",
        "\n",
        "def _handle_auto_toggle(change: Dict[str, Any]) -> None:\n",
        "    if change.get(\"new\"):\n",
        "        _start_artifact_poll()\n",
        "    else:\n",
        "        _stop_artifact_poll(from_toggle=True)\n",
        "\n",
        "\n",
        "def _start_artifact_poll() -> None:\n",
        "    run_id = run_id_input.value.strip() or _artifact_viewer_run_id()\n",
        "    if not run_id:\n",
        "        status_label.value = _format_status_html(\"Set a Run ID before enabling auto-refresh.\", ok=False)\n",
        "        auto_refresh_toggle.value = False\n",
        "        return\n",
        "    loop = asyncio.get_event_loop()\n",
        "\n",
        "    async def _poll() -> None:\n",
        "        try:\n",
        "            async with httpx.AsyncClient(timeout=30.0) as client:\n",
        "                while auto_refresh_toggle.value:\n",
        "                    stage = stage_input.value.strip()\n",
        "                    active_run_id = run_id_input.value.strip() or _artifact_viewer_run_id()\n",
        "                    if not active_run_id:\n",
        "                        status_label.value = _format_status_html(\"Run ID cleared; stopping auto-refresh.\", ok=False)\n",
        "                        _stop_artifact_poll()\n",
        "                        return\n",
        "                    try:\n",
        "                        payload = await _fetch_artifacts_async(client, active_run_id, stage)\n",
        "                    except httpx.HTTPError as exc:\n",
        "                        status_label.value = _format_status_html(f\"Auto-refresh failed: {exc}\", ok=False)\n",
        "                        _stop_artifact_poll()\n",
        "                        return\n",
        "                    except Exception as exc:\n",
        "                        status_label.value = _format_status_html(f\"Error: {exc}\", ok=False)\n",
        "                        _stop_artifact_poll()\n",
        "                        return\n",
        "                    _render_artifacts(payload)\n",
        "                    status_label.value = _format_status_html(\"Auto-refresh OK.\", ok=True)\n",
        "                    interval = max(2.0, float(interval_input.value or 4.0))\n",
        "                    await asyncio.sleep(interval)\n",
        "        except asyncio.CancelledError:\n",
        "            status_label.value = _format_status_html(\"Auto-refresh stopped.\", ok=True)\n",
        "\n",
        "    _stop_artifact_poll(from_toggle=True)\n",
        "    artifact_viewer_state[\"task\"] = loop.create_task(_poll())\n",
        "\n",
        "\n",
        "run_id_input = widgets.Text(\n",
        "    value=_artifact_viewer_run_id(),\n",
        "    description=\"Run ID\",\n",
        "    placeholder=\"production run id\",\n",
        "    layout=widgets.Layout(width=\"50%\"),\n",
        ")\n",
        "stage_input = widgets.Text(\n",
        "    value=\"finalize\",\n",
        "    description=\"Stage filter\",\n",
        "    placeholder=\"finalize (default)\",\n",
        "    tooltip=\"Set to finalize to inspect final artifacts.\",\n",
        "    layout=widgets.Layout(width=\"45%\"),\n",
        ")\n",
        "refresh_button = widgets.Button(description=\"Refresh\", icon=\"refresh\")\n",
        "auto_refresh_toggle = widgets.ToggleButton(description=\"Auto-refresh\", icon=\"repeat\", value=False)\n",
        "interval_input = widgets.BoundedFloatText(value=4.0, min=2.0, max=60.0, step=1.0, description=\"Interval (s)\")\n",
        "status_label = widgets.HTML(value=_format_status_html(\"Idle\", ok=True))\n",
        "artifacts_output = widgets.Output(\n",
        "    layout=widgets.Layout(border=\"1px solid #ddd\", min_height=\"160px\", max_height=\"360px\", overflow=\"auto\")\n",
        ")\n",
        "\n",
        "refresh_button.on_click(_handle_manual_refresh)\n",
        "auto_refresh_toggle.observe(_handle_auto_toggle, names=\"value\")\n",
        "\n",
        "controls_row_1 = widgets.HBox([run_id_input, stage_input])\n",
        "controls_row_2 = widgets.HBox([refresh_button, auto_refresh_toggle, interval_input, status_label])\n",
        "artifacts_viewer_panel = widgets.VBox([controls_row_1, controls_row_2, artifacts_output])\n",
        "\n",
        "display(artifacts_viewer_panel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae9d3ec3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sync artifacts viewer Run ID widget with control panel\n",
        "if \"control_panel\" in globals() and hasattr(control_panel, \"run_id_input\"):\n",
        "    run_id_input.value = control_panel.run_id_input.value\n",
        "print(\"Artifacts viewer run_id now:\", run_id_input.value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cdbbd2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trigger a manual artifacts refresh for verification logs\n",
        "_handle_manual_refresh(None)\n",
        "print(\"Artifacts viewer manual refresh invoked.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00e2e052",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Capture artifacts payload for notebook log (single fetch)\n",
        "import json\n",
        "current_run = run_id_input.value.strip() or _artifact_viewer_run_id()\n",
        "payload_snapshot = _fetch_artifacts_sync(current_run, stage_input.value.strip())\n",
        "stage_names = []\n",
        "for stage in payload_snapshot.get(\"stages\", []):\n",
        "    stage_names.append(stage.get(\"stage\") or stage.get(\"stage_id\"))\n",
        "print(json.dumps(\n",
        "    {\n",
        "        \"run_id\": current_run,\n",
        "        \"artifact_count\": len(list(_iter_artifacts(payload_snapshot))),\n",
        "        \"stages\": stage_names,\n",
        "    },\n",
        "    indent=2,\n",
        "    ensure_ascii=False,\n",
        "))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27f9465d",
      "metadata": {},
      "source": [
        "## Filesystem artifact retention helper\n",
        "Use this cell to trim `ARTIFACTS_FS_ROOT` when running the filesystem shim. It validates backend settings, shells out to `scripts/filesystem_artifacts.py prune`, streams logs inline, and defaults to dry-run mode so you can inspect the plan before deleting artifacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d1b902f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shlex\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "import ipywidgets as widgets\n",
        "\n",
        "REPO_ROOT = Path.cwd().resolve()\n",
        "CLI_PATH = REPO_ROOT / \"scripts\" / \"filesystem_artifacts.py\"\n",
        "\n",
        "root_default = os.environ.get(\"ARTIFACTS_FS_ROOT\", \"\")\n",
        "index_default = os.environ.get(\"ARTIFACTS_FS_INDEX\", \"\")\n",
        "backend_value = os.environ.get(\"ARTIFACTS_BACKEND\", \"\")\n",
        "\n",
        "root_input = widgets.Text(value=root_default, description=\"FS root\", layout=widgets.Layout(width=\"60%\"))\n",
        "index_input = widgets.Text(value=index_default, description=\"Index\", layout=widgets.Layout(width=\"60%\"))\n",
        "max_bytes_input = widgets.Text(value=\"200g\", description=\"Max bytes\")\n",
        "max_age_input = widgets.BoundedFloatText(value=14.0, min=0.0, max=365.0, step=1.0, description=\"Max age (days)\")\n",
        "min_free_input = widgets.Text(value=\"\", description=\"Min free\")\n",
        "runs_input = widgets.Textarea(\n",
        "    value=\"\",\n",
        "    placeholder=\"Optional run IDs (one per line)\",\n",
        "    description=\"Run filter\",\n",
        "    layout=widgets.Layout(width=\"60%\", height=\"80px\"),\n",
        ")\n",
        "dry_run_checkbox = widgets.Checkbox(value=True, description=\"Dry run only\", indent=False)\n",
        "assume_yes_checkbox = widgets.Checkbox(value=False, description=\"Auto confirm (--yes)\", indent=False)\n",
        "run_button = widgets.Button(description=\"Run prune\", icon=\"trash\")\n",
        "status_label = widgets.HTML(\"\")\n",
        "log_output = widgets.Output(layout=widgets.Layout(border=\"1px solid #ccc\", max_height=\"260px\", overflow=\"auto\"))\n",
        "\n",
        "\n",
        "def _format_status(message: str, *, ok: bool) -> str:\n",
        "    color = \"#2e7d32\" if ok else \"#c62828\"\n",
        "    return f\"<span style='color:{color}; font-weight:bold'>{message}</span>\"\n",
        "\n",
        "\n",
        "def _build_command() -> List[str]:\n",
        "    cmd = [sys.executable, str(CLI_PATH), \"prune\"]\n",
        "    root_value = root_input.value.strip()\n",
        "    index_value = index_input.value.strip()\n",
        "    if root_value:\n",
        "        cmd.extend([\"--root\", root_value])\n",
        "    if index_value:\n",
        "        cmd.extend([\"--index\", index_value])\n",
        "    if max_bytes_input.value.strip():\n",
        "        cmd.extend([\"--max-bytes\", max_bytes_input.value.strip()])\n",
        "    if max_age_input.value and max_age_input.value > 0:\n",
        "        cmd.extend([\"--max-age-days\", str(max_age_input.value)])\n",
        "    if min_free_input.value.strip():\n",
        "        cmd.extend([\"--min-free-bytes\", min_free_input.value.strip()])\n",
        "    for run_id in runs_input.value.splitlines():\n",
        "        run_id = run_id.strip()\n",
        "        if run_id:\n",
        "            cmd.extend([\"--run\", run_id])\n",
        "    if dry_run_checkbox.value:\n",
        "        cmd.append(\"--dry-run\")\n",
        "    else:\n",
        "        cmd.append(\"--no-dry-run\")\n",
        "    if assume_yes_checkbox.value:\n",
        "        cmd.append(\"--yes\")\n",
        "    return cmd\n",
        "\n",
        "\n",
        "def _run_prune(_btn: widgets.Button) -> None:\n",
        "    log_output.clear_output()\n",
        "    errors = []\n",
        "    effective_backend = (backend_value or os.environ.get(\"ARTIFACTS_BACKEND\", \"\")).lower()\n",
        "    if effective_backend != \"filesystem\":\n",
        "        errors.append(\"Set ARTIFACTS_BACKEND=filesystem before pruning.\")\n",
        "    if not CLI_PATH.exists():\n",
        "        errors.append(f\"Missing CLI script: {CLI_PATH}\")\n",
        "    if not (max_bytes_input.value.strip() or max_age_input.value > 0 or min_free_input.value.strip()):\n",
        "        errors.append(\"Provide at least one retention constraint (max bytes / age / min free).\")\n",
        "    if errors:\n",
        "        status_label.value = _format_status(\" \".join(errors), ok=False)\n",
        "        return\n",
        "\n",
        "    cmd = _build_command()\n",
        "    env = os.environ.copy()\n",
        "    pythonpath_bits = [str(REPO_ROOT), str(REPO_ROOT / \"src\")]\n",
        "    if env.get(\"PYTHONPATH\"):\n",
        "        pythonpath_bits.append(env[\"PYTHONPATH\"])\n",
        "    env[\"PYTHONPATH\"] = os.pathsep.join(pythonpath_bits)\n",
        "\n",
        "    status_label.value = _format_status(\"Running prune command…\", ok=True)\n",
        "\n",
        "    with log_output:\n",
        "        print(\"$\", \" \".join(shlex.quote(part) for part in cmd))\n",
        "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, env=env)\n",
        "        assert process.stdout is not None\n",
        "        for line in process.stdout:\n",
        "            print(line.rstrip())\n",
        "        return_code = process.wait()\n",
        "\n",
        "    if return_code == 0:\n",
        "        status_label.value = _format_status(\"Prune command completed.\", ok=True)\n",
        "    else:\n",
        "        status_label.value = _format_status(f\"Prune command failed (exit {return_code}).\", ok=False)\n",
        "\n",
        "\n",
        "run_button.on_click(_run_prune)\n",
        "\n",
        "prune_controls = widgets.VBox(\n",
        "    [\n",
        "        widgets.HBox([root_input, index_input]),\n",
        "        widgets.HBox([max_bytes_input, max_age_input, min_free_input]),\n",
        "        runs_input,\n",
        "        widgets.HBox([dry_run_checkbox, assume_yes_checkbox, run_button]),\n",
        "        status_label,\n",
        "        log_output,\n",
        "    ]\n",
        ")\n",
        "\n",
        "display(prune_controls)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35de9b1a",
      "metadata": {},
      "source": [
        "## Optional: run a stub orchestration smoke test\n",
        "This cell runs the Python runner in simulation mode (fallback adapters) so you can confirm Drive folders are writable before enabling real models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cfc4070",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Optional orchestrator smoke run (uses fallback adapters)\n",
        "from pathlib import Path\n",
        "from sparkle_motion.orchestrator import Runner\n",
        "\n",
        "workspace_root = DRIVE_ROOT if USE_DRIVE else LOCAL_ROOT\n",
        "runs_root = workspace_root / \"runs\"\n",
        "runs_root.mkdir(parents=True, exist_ok=True)\n",
        "movie_plan = {\n",
        "    \"title\": \"Colab Smoke\",\n",
        "    \"shots\": [\n",
        "        {\n",
        "            \"id\": \"shot_001\",\n",
        "            \"visual_description\": \"Test scene\",\n",
        "            \"duration_sec\": 2.0,\n",
        "            \"dialogue\": [{\"character\": \"narrator\", \"text\": \"Hello from Colab\"}],\n",
        "        }\n",
        "    ],\n",
        "}\n",
        "runner = Runner(runs_root=str(runs_root))\n",
        "asset_refs = runner.run(movie_plan=movie_plan, run_id=\"colab_smoke\", resume=True)\n",
        "print(\"Smoke run complete. Final asset refs keys:\", asset_refs.keys())\n",
        "print(\"Runs directory:\", runs_root)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sparkle_motion",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
